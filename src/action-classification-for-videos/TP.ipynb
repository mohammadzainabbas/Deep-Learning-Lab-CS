{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3IqyMbhox2l"
      },
      "source": [
        "# TP6: Action Classification via Recurrent models in Pytorch\n",
        "\n",
        "**Authors:** \n",
        "- julien.denize@centralesupelec.fr\n",
        "- tom.dupuis@centralesupelec.fr\n",
        "\n",
        "\n",
        "If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n",
        "\n",
        "For this tp you need to have some ground understanding of pytorch and basics introduction. It is available [here](https://pytorch.org/tutorials/beginner/basics/intro.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHxQTYAGo6gX"
      },
      "source": [
        "## Objective\n",
        "\n",
        "In this TP, we will implement various Recurrent Models to make action classification for videos on the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php).\n",
        "\n",
        "UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories.\n",
        "\n",
        "With 13320 videos from 101 action categories, UCF101 gives diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it was the most challenging data set few years ago. \n",
        "\n",
        "We will only use 20 classes from UCF101 to speed-up training.\n",
        "\n",
        "We will extract frame features using a CNN backbone and make action classification on top of these features using various models:\n",
        "- a custom LSTM model\n",
        "- a RNN Pytorch model\n",
        "- a LSTM Pytorch model\n",
        "- a GRU Pytorch model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCDb00TSCJp2"
      },
      "source": [
        "## Your task\n",
        "\n",
        "Fill the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZO7U2GCzxo2e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Seed everything\n",
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUcRt_38CLtc"
      },
      "source": [
        "## Retrieve the UCF101 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPZYHsftDczF"
      },
      "source": [
        "### Download the UCF101 dataset frames from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhTdf_bECrzp"
      },
      "source": [
        "We will download the dataset from Kaggle. On Kaggle, UCF101 is stored as JPEG frames from each video clips.\n",
        "\n",
        "Make sure to have correctly setup your drive following this procedure: https://www.kaggle.com/general/156610"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAWmmemqovQ2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/gdrive')\n",
        "files.upload() #this will prompt you to upload the kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaiXb7s2yXUt"
      },
      "source": [
        "The following cell will download the zip dataset containing all videos from the UCF101 dataset.\n",
        "\n",
        "It should take around 1-2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE-cydBbDS5F"
      },
      "outputs": [],
      "source": [
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d pevogam/ucf101-frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLuPajcydoH"
      },
      "source": [
        "The following cell only extract the 20 first classes of UCF101. Each frame from each video is located in its class folder.\n",
        "\n",
        "It should take around 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXr3-w1ZExuU"
      },
      "outputs": [],
      "source": [
        "classes_to_extract = [\"ApplyEyeMakeup\", \"ApplyLipstick\", \"Archery\", \n",
        "                      \"BabyCrawling\", \"BalanceBeam\", \"BandMarching\", \n",
        "                      \"BaseballPitch\", \"Basketball\", \"BasketballDunk\", \n",
        "                      \"BenchPress\", \"Biking\", \"Billiards\", \"BlowDryHair\",\n",
        "                      \"BlowingCandles\", \"BodyWeightSquats\", \"Bowling\", \"BoxingPunchingBag\",\n",
        "                      \"BoxingSpeedBag\", \"BreastStroke\", \"BrushingTeeth\",]\n",
        "\n",
        "idx_to_class = {i: class_ for i,class_ in enumerate(classes_to_extract)}\n",
        "\n",
        "!mkdir -p ucf101/{train,test}\n",
        "\n",
        "for class_ in classes_to_extract:\n",
        "  !unzip -qq ucf101-frames.zip \"train/\"{class_}\"/*\" -d \"/content/ucf101/\"\n",
        "  !unzip -qq ucf101-frames.zip \"test/\"{class_}\"/*\" -d \"/content/ucf101/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HND7GHtZykPK"
      },
      "source": [
        "The following cell will store the name of each video associated to the number of frames and its label in a csv for the train and test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eFegS9JIpp8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "classes_to_extract =  [\"ApplyEyeMakeup\", \"ApplyLipstick\", \"Archery\", \n",
        "                      \"BabyCrawling\", \"BalanceBeam\", \"BandMarching\", \n",
        "                      \"BaseballPitch\", \"Basketball\", \"BasketballDunk\", \n",
        "                      \"BenchPress\", \"Biking\", \"Billiards\", \"BlowDryHair\",\n",
        "                      \"BlowingCandles\", \"BodyWeightSquats\", \"Bowling\", \"BoxingPunchingBag\",\n",
        "                      \"BoxingSpeedBag\", \"BreastStroke\", \"BrushingTeeth\",]\n",
        "idx_to_class = {i: class_ for i,class_ in enumerate(classes_to_extract)}\n",
        "\n",
        "# Iterate over the train and test splits.\n",
        "for split in [\"train\", \"test\"]:\n",
        "  split_dir = f\"/content/ucf101/{split}/\"\n",
        "  # dictionary to store dataframe data.\n",
        "  split_dict = {\"video\": [], \"class\": [], \"num_frames\": []}\n",
        "  for idx_class, class_ in enumerate(classes_to_extract):\n",
        "\n",
        "    class_dir = f\"{split_dir}{class_}/\"\n",
        "\n",
        "    # All frames for the class.\n",
        "    files = [f for f in os.listdir(class_dir) if os.path.isfile(f\"{class_dir}{f}\")]\n",
        "\n",
        "    # Pattern to retrieve the video name from a JPEG file.\n",
        "    video_pattern = r\"^.+\\/(.+)-[\\d]+\\.jpg\"\n",
        "\n",
        "    videos = {}\n",
        "    for f in files:\n",
        "      path_file = f\"{class_dir}{f}\"\n",
        "      match = re.search(video_pattern, path_file).group(1)\n",
        "      \n",
        "      if match in videos:\n",
        "        videos[match] += 1\n",
        "      else:\n",
        "        videos[match] = 0\n",
        "    \n",
        "    for video, num_frames in videos.items():\n",
        "      split_dict[\"video\"].append(f\"{class_}/{video}\")\n",
        "      split_dict[\"class\"].append(idx_class)\n",
        "      split_dict[\"num_frames\"].append(num_frames)\n",
        "  \n",
        "  dataframe = pd.DataFrame.from_dict(split_dict)\n",
        "  # Store in csv files.\n",
        "  dataframe.to_csv(f\"/content/ucf101/{split}.csv\", index=False, header=False)       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwESu_LW8H0M"
      },
      "source": [
        "## Make a dataset for the frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZb2bBvZ917s"
      },
      "source": [
        "In this section, we will implement a dataset that can read the data from our UCF101 dataset stored as frames thanks to the csv files previously created. A sample corresponds to a unique video. This means that getting from the dataset a speicifed index returns the whole video and not a specific frame of a video.\n",
        "\n",
        "```python\n",
        "dataset = FrameDataset(csv_file)\n",
        "x, y = dataset[100] # returns the loaded frames of the whole video and its class.\n",
        "```\n",
        "\n",
        "Inside the frame dataset we need to transform the loaded image to first resize the shorter side to $256$ and then CenterCrop the image to a square image of $224 \\times 224$. Finally the image should be cast to tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPTb_ubk8HOQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from typing import Any, Tuple\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FrameDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Frame dataset from a CSV file that decode frames to fetch videos.\n",
        "\n",
        "  Args:\n",
        "    csv_file: the csv file that contains information about the dataset.\n",
        "    video_path_prefix: the root folder of the videos.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, csv_file: str, video_path_prefix: str) -> None:\n",
        "    super().__init__()\n",
        "    self.video_path_prefix = Path(video_path_prefix)\n",
        "\n",
        "    self._load_csv(csv_file)\n",
        "\n",
        "  def _load_csv(self, csv_file: str) -> None:\n",
        "    # --- START CODE HERE (01)\n",
        "    # Read the csv file using the dataframe from pandas.\n",
        "    dataframe =\n",
        "    # --- END CODE HERE\n",
        "    self.data = dataframe\n",
        "\n",
        "    # --- START CODE HERE (02)\n",
        "    # Instantiate the transformation to apply on each image.\n",
        "    self.transform = \n",
        "    # --- END CODE HERE\n",
        "  \n",
        "  def __getitem__(self, idx: int) -> Tuple[Any]:\n",
        "      row = self.data.loc[idx]\n",
        "      video_name, label, num_frames = row \n",
        "\n",
        "      frames = [None for _ in range(num_frames)]\n",
        "\n",
        "      # Load all the frames of the video.\n",
        "      for i in range(1, num_frames + 1):\n",
        "        # --- START CODE HERE (03)\n",
        "        # Load the frame and store it in the frames list as a tensor.\n",
        "        path =\n",
        "        with open(path, \"rb\") as f:\n",
        "          img = \n",
        "          frames[i - 1] = \n",
        "        # --- END CODE HERE\n",
        "      \n",
        "      frames = torch.stack(frames)\n",
        "\n",
        "      return frames, label\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.data)\n",
        "\n",
        "dataset = FrameDataset(\"/content/ucf101/train.csv\", \"/content/ucf101/train/\")\n",
        "frames, label = dataset[0]\n",
        "frames.shape, label # Should return (torch.Size([212, 3, 224, 224]), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-dAOemFw-F2"
      },
      "source": [
        "### Visualize the videos\n",
        "\n",
        "Below we provide code to visualize the data using matplotlib and animations sliding through all frames of videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4OQ8IpBxDvk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from torch import Tensor\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "def display_video(video: Tensor) -> animation.ArtistAnimation:\n",
        "  \"\"\"\n",
        "  Get the animation to display the video.\n",
        "\n",
        "  Args:\n",
        "    video: the video to display.\n",
        "\n",
        "  Return:\n",
        "    the slide show of for the video.\n",
        "  \"\"\"\n",
        "  video = (video.permute((0, 2, 3, 1)) * 255.).long()\n",
        "  fig, ax = plt.subplots()\n",
        "  frames = [[ax.imshow(video[i])] for i in range(len(video))]\n",
        "  ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "  anim = animation.ArtistAnimation(fig,frames)\n",
        "  plt.close()\n",
        "  return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjiqwuzhzZRS"
      },
      "outputs": [],
      "source": [
        "anim = display_video(frames[:32]) # To prevent the notebook from failing don't display all frames\n",
        "anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zffOoZFKDhZn"
      },
      "source": [
        "## Extract the features of each video from a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc98djS3wUcT"
      },
      "source": [
        "We will implement a feature extractor. To do so we need to:\n",
        "- initialize a CNN 2D model with pretrained weights.\n",
        "- iterate over the FrameDataset to extract features from each video for each frame.\n",
        "- store the features to reuse them afterwards and not having to compute them everytime we reload the notebook (the extraction takes 30 minutes, don't worry we will provide you the saved extracted features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDJhac5ZzDsS"
      },
      "source": [
        "### Retrieve the model\n",
        "\n",
        "We will use the Resnet18 backbone to extract our features from torchvision with its ImageNet pretrained weights.\n",
        "\n",
        "To extract features and not class predictions, we will replace the last fully connected layer by the identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOWVwj7SB8FC"
      },
      "outputs": [],
      "source": [
        "# --- START CODE HERE (04)\n",
        "# Import the resnet model and the Identity module.\n",
        "\n",
        "# --- END CODE HERE\n",
        "\n",
        "def get_pretrained_model(model_name: str = \"resnet18\"):\n",
        "  if model_name == \"resnet18\":\n",
        "    # --- START CODE HERE (05)\n",
        "    # Instantiate the model and replace the last fully connected layer.\n",
        "    model =\n",
        "\n",
        "    # --- END CODE HERE\n",
        "  else:\n",
        "    raise NotImplementedError(f\"{model_name} is not supported.\")\n",
        "\n",
        "  return model\n",
        "\n",
        "get_pretrained_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TddPXdNo1mmb"
      },
      "source": [
        "### Feature extractor\n",
        "\n",
        "\n",
        "You will now implement the feature extractor. We will concatenate the features of each video in one big tensor and output three different tensors:\n",
        "- the features\n",
        "- the labels\n",
        "- the number of frames per video.\n",
        "\n",
        "To store the features in one big tensor, we need to:\n",
        "- retrieve the maximum number of frames across the dataset.\n",
        "- for each video, extract the features of each image, stack the features and pad the features frame dimension to match the maximum number of frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuTnvLJm1Y_e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch import Tensor\n",
        "from torch.nn import Module\n",
        "\n",
        "@torch.no_grad()\n",
        "def feature_extractor(dataset: FrameDataset, model: Module, device: str = \"cpu\") -> Tensor:\n",
        "  \"\"\"\n",
        "  Extract the features from a video dataset.\n",
        "\n",
        "  Args:\n",
        "    dataset: The dataset to extract features from.\n",
        "    model: The model to make the extraction.\n",
        "    device: The device on which to perform extraction.\n",
        "  \n",
        "  Return:\n",
        "    The features for each video along with the labels and the number of frames per video.\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  features = [None for _ in range(len(dataset))]\n",
        "\n",
        "  # --- START CODE HERE (06)\n",
        "  # Retrieve the maximum number of frames and the padding number of frames to apply for each video.\n",
        "  max_num_frames =\n",
        "  diff_num_frames =\n",
        "  # --- END CODE HERE\n",
        "\n",
        "  for i in range(len(dataset)):\n",
        "    frames, y = dataset[i]\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "      frames = frames.cuda(non_blocking=True)\n",
        "    \n",
        "    # --- START CODE HERE (07)\n",
        "    # Extract the frame features and pad if necessary new frames as zeros.\n",
        "    features[i] =\n",
        "    if:\n",
        "      \n",
        "      features[i] =\n",
        "    # --- END CODE HERE\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(f\"{i+1}/{len(dataset)} videos extracted.\")\n",
        "  \n",
        "  features = torch.stack(features)\n",
        "  labels = torch.tensor(dataset.data.iloc[:, -2])\n",
        "  num_frames = torch.tensor(dataset.data.iloc[:, -1])\n",
        "  \n",
        "  return features, labels, num_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HFn7aCi1aHh"
      },
      "source": [
        "### [Optional] Extract the features\n",
        "\n",
        "It takes around 35 minutes to extract all the features. We suggest you to retrieve them from our drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg7C5na8Q53H"
      },
      "outputs": [],
      "source": [
        "train_frame_dataset = FrameDataset(\"/content/ucf101/train.csv\", \"/content/ucf101/train/\")\n",
        "test_frame_dataset = FrameDataset(\"/content/ucf101/test.csv\", \"/content/ucf101/test/\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = get_pretrained_model(\"resnet18\").cuda()\n",
        "\n",
        "train_features, train_labels, train_num_frames = feature_extractor(train_frame_dataset, model, device)\n",
        "test_features, test_labels, test_num_frames = feature_extractor(test_frame_dataset, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAts1XUEvF2p"
      },
      "source": [
        "### [Optional] Save the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULC9CSMDSbqi"
      },
      "outputs": [],
      "source": [
        "del train_frame_dataset\n",
        "del test_frame_dataset\n",
        "del model\n",
        "\n",
        "path_to_save_features = \"/content/gdrive/MyDrive/COURS CS BDMA/Cours Deep Learning/Cours 6/features/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT3wbjI3XZ1J"
      },
      "outputs": [],
      "source": [
        "torch.save(train_features, f\"{path_to_save_features}train_features.pt\")\n",
        "torch.save(train_labels, f\"{path_to_save_features}train_labels.pt\")\n",
        "torch.save(train_num_frames, f\"{path_to_save_features}train_num_frames.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R0n2KH-XQFx"
      },
      "outputs": [],
      "source": [
        "torch.save(test_features, f\"{path_to_save_features}test_features.pt\")\n",
        "torch.save(test_labels, f\"{path_to_save_features}test_labels.pt\")\n",
        "torch.save(test_num_frames, f\"{path_to_save_features}test_num_frames.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8CnP58MvD1C"
      },
      "source": [
        "## Make Dataset from extracted features\n",
        "\n",
        "Now that we extracted the features from the dataset, we can now iterate over features data to perform training of various models, quicker than if we kept the different frames.\n",
        "\n",
        "If you didn't extract the features yourself, you can find them from this google drive folder:\n",
        "\n",
        "https://drive.google.com/drive/folders/1-35TDMEpv-J5ca7t45ZZ7nXSrVFshOZp?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBHStovDzmAk"
      },
      "source": [
        "### Make clip samplers\n",
        "\n",
        "To make use of our data in different models, we need to provide sequence of the same length. For that we will implement two clip samplers:\n",
        "- a random clip sampler that randomly subsample a sequence among the video.\n",
        "- a middle clip sampler that select the sequence in the middle of the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkC9vn5uxTA4"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "\n",
        "def random_clip_sampler(features: Tensor, sequence_length: int = 32) -> Tensor:\n",
        "  \"\"\"\n",
        "  Random clip sampler that randomly select a sub sequence in the video.\n",
        "  \n",
        "  Args:\n",
        "    features: the video features to extract the sequence from. Shape T, C, H, W.\n",
        "    sequence_length: the size of the sequence to extract.\n",
        "  \n",
        "  Return:\n",
        "    The extracted sequence.\n",
        "  \"\"\"\n",
        "\n",
        "  # --- START CODE HERE (08)\n",
        "  # Compute max start sequence indice and sample randomly the start indice for the sequence.\n",
        "  max_start_sequence =\n",
        "  start =\n",
        "  # --- END CODE HERE\n",
        "\n",
        "  return features[start: start + sequence_length]\n",
        "\n",
        "def middle_clip_sampler(features: Tensor, sequence_length: int = 32) -> Tensor:\n",
        "  \"\"\"\n",
        "  Clip sampler that selects1 the middle sequence in the video.\n",
        "  \n",
        "  Args:\n",
        "    features: the video features to extract the sequence from. Shape T, C, H, W.\n",
        "    sequence_length: the size of the sequence to extract.\n",
        "  \n",
        "  Return:\n",
        "    The extracted sequence.\n",
        "  \"\"\"\n",
        "\n",
        "  # --- START CODE HERE (09)\n",
        "  # Compute the start and the end indices of the sequence.\n",
        "  start =\n",
        "  end =\n",
        "  # --- END CODE HERE\n",
        "\n",
        "  return features[start:end]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhCFhvmqzolu"
      },
      "source": [
        "### Instantiate Features datasets\n",
        "\n",
        "We will implement a dataset that read from files the features, labels and num frames that we saved after extraction.\n",
        "\n",
        "For that we will only keep videos that have enough number of frames given the sequence length we will provide (by default 32).\n",
        "\n",
        "Then for each video the dataset returns the tuple of:\n",
        "- the sequence features extracted from the clip sampler.\n",
        "- the label of the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG-fCBNSvN9t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Any, Callable, Tuple\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FeaturesDataset(Dataset):\n",
        "  def __init__(self, root_folder: str, split: str, clip_sampler: Callable, sequence_length: int = 32) -> None:\n",
        "    \"\"\"\n",
        "    Features dataset from several torch files that decode features for videos.\n",
        "\n",
        "    Args:\n",
        "      root_folder: the folder that contaiins the dataset.\n",
        "      split: the split of the dataset. Either 'train' or 'test'.\n",
        "      clip_sampler: the clip sampler to extract sequences from videos.\n",
        "      sequence_length: sequence length for each clip.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.features = torch.load(f\"{root_folder}/{split}_features.pt\")\n",
        "    self.labels = torch.load(f\"{root_folder}/{split}_labels.pt\")\n",
        "    self.num_frames = torch.load(f\"{root_folder}/{split}_num_frames.pt\")\n",
        "\n",
        "    self.clip_sampler = clip_sampler\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "    # --- START CODE HERE (09)\n",
        "    # Only keep elements that have a number of frames larger than the sequence length.\n",
        "    keep_indices =\n",
        "    self.features =\n",
        "    self.labels =\n",
        "    self.num_frames =\n",
        "    # --- END CODE HERE\n",
        "  \n",
        "  def __getitem__(self, idx: int) -> Tuple[Any]:\n",
        "    num_frames = self.num_frames[idx]\n",
        "    label = self.labels[idx]\n",
        "    \n",
        "    # --- START CODE HERE (10)\n",
        "    # Only keep frames that have not been padded and sample a clip.\n",
        "    features =\n",
        "    features =\n",
        "    # --- END CODE HERE\n",
        "    \n",
        "    return features, label\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.features.shape[0]\n",
        "\n",
        "root_folder = \"/content/gdrive/MyDrive/COURS CS BDMA/Cours Deep Learning/Cours 6/features/\"\n",
        "\n",
        "# To avoid RAM shortage we delete the train and test dataset before loading them again (if they were loaded).\n",
        "try:\n",
        "  del train_dataset\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "try: \n",
        "  test_dataset\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "train_dataset = FeaturesDataset(root_folder, \"train\", random_clip_sampler)\n",
        "test_dataset = FeaturesDataset(root_folder, \"test\", middle_clip_sampler)\n",
        "frames, label = train_dataset[0]\n",
        "frames.shape, label # Should return (torch.Size([32, 512]), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPhmFFR7zsBv"
      },
      "source": [
        "### Instantiate dataloaders\n",
        "\n",
        "We instantiate the train and test dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nogiVrCzXXG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              pin_memory=device==\"cuda\", num_workers=1,\n",
        "                              drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                             pin_memory=device==\"cuda\", num_workers=1,\n",
        "                             drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTizqeo900rx"
      },
      "source": [
        "## Make the classification models\n",
        "\n",
        "We will make action classification from three different models:\n",
        "- a custom LSTM model.\n",
        "- a RNN model.\n",
        "- a LSTM model.\n",
        "- a GRU model.\n",
        "\n",
        "We expect to have the following performance order: RNN < LSTM < GRU.\n",
        "\n",
        "However, due to the dataset size we don't observe a drastic difference between the methods. Still in average we should observe this performance order between the methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7c1QR7qsUcw"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Below is defined the various functions to perform training already filled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8vDj50ksWjG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_losses_and_accuracies(train_loss, test_loss, train_accuracy, test_accuracy):\n",
        "  \"\"\"\n",
        "  Display the training and testing loss and accuracies.\n",
        "\n",
        "  Args:\n",
        "    train_loss: the training loss list.\n",
        "    test_loss: the testing loss list.\n",
        "    train_accuracy: the training accuracy list.\n",
        "    test_accuracy: the testing accuracy list.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(train_loss, 'r')\n",
        "  plt.plot(test_loss, 'g--')\n",
        "  plt.xlabel('# epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(train_accuracy, 'r')\n",
        "  plt.plot(test_accuracy, 'g--')\n",
        "  plt.xlabel('# epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "def F_computeAccuracy(preds, y):\n",
        "    \"\"\"Compute the accuracy.\n",
        "    \n",
        "    Args:\n",
        "      preds:\n",
        "          predicted value by the MLP.\n",
        "      y:\n",
        "          ground-truth class to predict.\n",
        "    \n",
        "    Return:\n",
        "      The accuracy.\n",
        "    \"\"\"\n",
        "    \n",
        "    m = y.shape[0]\n",
        "    if preds.shape[1] == 1:\n",
        "      hard_preds = (preds > 0.5).to(torch.int64)\n",
        "    else:\n",
        "      hard_preds = (torch.argmax(preds, 1)).to(torch.int64)\n",
        "    return torch.sum(hard_preds==y) / m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZO81nIfsvlT"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, num_epochs, device=\"cpu\"):\n",
        "  \"\"\"\n",
        "  Make the training loop.\n",
        "\n",
        "  Args:\n",
        "    model: the model to train.\n",
        "    loss_fn: the loss to minimize.\n",
        "    optimizer: the optimizer for the model.\n",
        "    train_dataloader: the train dataloader.\n",
        "    test_dataloader: the test dataloader.\n",
        "    num_epochs: the number of epochs to train.\n",
        "    device: the device to train on.\n",
        "  \n",
        "  Return:\n",
        "    The accuracy and loss metrics.\n",
        "  \"\"\"\n",
        "\n",
        "  train_loss_epoch, train_accuracy_epoch = [], []\n",
        "  test_loss_epoch, test_accuracy_epoch = [], []\n",
        "\n",
        "  for num_epoch in range(1, nb_epoch + 1):\n",
        "    model.train()\n",
        "    train_loss_iter, train_accuracy_iter = [], []\n",
        "    test_loss_iter, test_accuracy_iter = [], []\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "      \n",
        "      # --- Load train data\n",
        "      X, y = batch\n",
        "      if device == \"cuda\":\n",
        "        X, y = X.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
        "      \n",
        "      # --- Forward\n",
        "      train_preds = model(X)\n",
        "      loss = loss_fn(train_preds, y)\n",
        "\n",
        "      accuracy = F_computeAccuracy(train_preds, y)\n",
        "      \n",
        "      # --- Store results on train\n",
        "      train_loss_iter.append(loss.item())\n",
        "      train_accuracy_iter.append(accuracy)\n",
        "      \n",
        "      # --- Backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "      # --- Update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      for batch_idx, batch in enumerate(test_dataloader):\n",
        "\n",
        "        # --- Load test data\n",
        "        X, y = batch\n",
        "        if device == \"cuda\":\n",
        "          X, y = X.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
        "\n",
        "        # --- Store results on test\n",
        "        test_preds = model(X)\n",
        "        loss = loss_fn(test_preds, y)\n",
        "        accuracy = F_computeAccuracy(test_preds, y)\n",
        "        test_loss_iter.append(loss.item())    \n",
        "        test_accuracy_iter.append(accuracy)\n",
        "\n",
        "    train_loss_epoch.append(torch.mean(torch.tensor(train_loss_iter)))  \n",
        "    train_accuracy_epoch.append(torch.mean(torch.tensor(train_accuracy_iter)))  \n",
        "    test_loss_epoch.append(torch.mean(torch.tensor(test_loss_iter)))  \n",
        "    test_accuracy_epoch.append(torch.mean(torch.tensor(test_accuracy_iter)))  \n",
        "    \n",
        "    print(\"epoch: {0:d}/{1:d} (loss: train {2:.2f} test {3:.2f}) (accuracy: train {4:.2f} test {5:.2f})\".format(num_epoch,\n",
        "                                                                                                                nb_epoch,\n",
        "                                                                                                                train_loss_epoch[-1],\n",
        "                                                                                                                test_loss_epoch[-1],\n",
        "                                                                                                                train_accuracy_epoch[-1],\n",
        "                                                                                                                test_accuracy_epoch[-1]))\n",
        "\n",
        "  return train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSBwzelQyhba"
      },
      "source": [
        "### Custom LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85JbkCC08T6"
      },
      "source": [
        "We will implement a LSTM model. It is an improved version of a RNN that implements a memory mechanism to avoid vanishing gradient and loss of long-term dependencies. \n",
        "\n",
        "![](https://miro.medium.com/max/1100/0*VXNy36Ay_Rq3m1LE.png)\n",
        "\n",
        "The forget gate equations are the following:\n",
        "\n",
        "$f_t = \\sigma(U_fx_t + V_fh_{t-1} + b_f)$\n",
        "\n",
        "$C'_t = f_t \\cdot C_{t-1}$\n",
        "\n",
        "The memory gate equations are the following:\n",
        "\n",
        "$i_t = \\sigma(U_ix_t + V_ih_{t-1} + b_i)$\n",
        "\n",
        "$g_t = tanh(U_cx_t+V_ch_{t-1}+b_c)$\n",
        "\n",
        "$C_t = C_t' + i_t \\cdot g_t$\n",
        "\n",
        "The output gate equations are the following:\n",
        "\n",
        "$o_t = \\sigma(U_0x_t + V_0h_{t-1} + b_0)$\n",
        "\n",
        "$h_t = o_t \\cdot tanh(C_t)$\n",
        "\n",
        "\n",
        "We will now implement a LSTM cell followed by a classifier with the following parameters:\n",
        "- input dimension 512 (feature size of resnet18).\n",
        "- hidden dimension 256.\n",
        "- output dimension 20 (number of classes).\n",
        "- the parameters W, U, bias will be only one big matrices that we will cut in pieces in forward of shape:\n",
        "  - $W$: input dimension, hidden dimension * 4.\n",
        "  - $U$: hidden dimension, hidden dimension * 4.\n",
        "  - $bias$: hidden dimension * 4.\n",
        "- a dropout layer after the LSTM cell of probability 0.8.\n",
        "- a linear layer for classification.\n",
        "\n",
        "\n",
        "In LSTM layers, all the hidden representations are in the output but because we only want to perform classification on the last state, we won't bother with that functionality.\n",
        "\n",
        "You should reach around 73-75 % test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAHMPvXhGLEh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Dropout, Linear\n",
        "import math\n",
        "\n",
        "class CustomLSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom LSTM model.\n",
        "\n",
        "    Args:\n",
        "      input_dim: the input dimension.\n",
        "      hidden_dim: the hidden dimension.\n",
        "      output_dim: the output dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int = 512, hidden_dim: int = 256, output_dim: int = 20):\n",
        "        super().__init__()\n",
        "        self.input_dim = hidden_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # --- START CODE HERE (11)\n",
        "        # Instantiate the W, U, bias matrice parameters aswell as the dropout and linear layers.\n",
        "        self.W =\n",
        "        self.U =\n",
        "        self.bias =\n",
        "\n",
        "        self.dropout =\n",
        "        self.fc =\n",
        "        # --- END CODE HERE\n",
        "  \n",
        "        self.init_weights()\n",
        "                \n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "         \n",
        "    def forward(self, x):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "\n",
        "        # --- START CODE HERE (12)\n",
        "        # Initalize h_t and c_t hidden states to zero tensors.\n",
        "        \n",
        "        # --- END CODE HERE\n",
        "         \n",
        "        self.hidden_dim\n",
        "        for t in range(sequence_length):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # --- START CODE HERE (13)\n",
        "            # Follow the LSTM equations\n",
        "\n",
        "            # batch the computations into a single matrix multiplication\n",
        "            \n",
        "\n",
        "              # i_t input gate\n",
        "              # f_t forget gate\n",
        "              # g_t\n",
        "              # o_t output gate\n",
        "            \n",
        "\n",
        "            # --- END CODE HERE\n",
        "        \n",
        "        # --- START CODE HERE (14)\n",
        "        # Apply the dropout and the classifier.\n",
        "\n",
        "        # --- END CODE HERE\n",
        "\n",
        "        return output\n",
        "\n",
        "CustomLSTMModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t-3It99uPlk"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Run over epochs\n",
        "nb_epoch = 50\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CustomLSTMModel().to(device)\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "loss_fn = CrossEntropyLoss().to(device)\n",
        "\n",
        "train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n",
        "  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z33f4tpMZxpW"
      },
      "outputs": [],
      "source": [
        "display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpDyNGnx1a3U"
      },
      "source": [
        "### RNN on top of the Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtP1DRrRGHpC"
      },
      "source": [
        "We will implement a RNN Model based on the pytorch [RNN layer](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
        "\n",
        "![](https://miro.medium.com/max/640/1*m8ATMpOZI5ttTjoWWXt4Lg.png)\n",
        "\n",
        "We will make use of the following layers:\n",
        "- a RNN layer with batch first parameter.\n",
        "- a Dropout layer of probability 0.8.\n",
        "- a linear classifier to make classifications.\n",
        "\n",
        "For the RNN layer, as the documentation states, we need to instantiate an initial hidden state. We will instantiate it to zero.\n",
        "\n",
        "You should reach 71-73% test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5yVgEpp1chq"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Dropout, Linear, Module, RNN\n",
        "from torch import nn\n",
        "\n",
        "class RNNModel(Module):\n",
        "    \"\"\"\n",
        "    RNN model.\n",
        "\n",
        "    Args:\n",
        "      input_dim: the input dimension.\n",
        "      output_dim: the output dimension.\n",
        "      hidden_dim: the hidden dimension.\n",
        "      n_layers: number of RNN layers stacked.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int = 512, output_dim: int = 20, hidden_dim: int = 512, n_layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # --- START CODE HERE (15)\n",
        "        # Instantiate the RNN, dropout and linear layers.\n",
        "        self.rnn =\n",
        "        self.dropout =\n",
        "        self.fc =\n",
        "        # --- END CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        batch_size = x.shape[0]        \n",
        "\n",
        "        # --- START CODE HERE (16)\n",
        "        # Initiliaze the hidden state to a zero tensor for the RNN layer.\n",
        "        # Retrieve last hidden state to apply dropout and linear classifier on.\n",
        "\n",
        "\n",
        "        # --- END CODE HERE\n",
        "        return output\n",
        "\n",
        "RNNModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ZN5rdYBCmY"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "# Run over epochs\n",
        "nb_epoch = 50\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = RNNModel().to(device)\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "loss_fn = CrossEntropyLoss().to(device)\n",
        "\n",
        "train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n",
        "  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cKDKIg6aBEp"
      },
      "outputs": [],
      "source": [
        "display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-vfgtSgBaPm"
      },
      "source": [
        "### Pytorch LSTM\n",
        "\n",
        "We will implement a LSTM Model based on the pytorch [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n",
        "\n",
        "We will make use of the following layers:\n",
        "- a LSTM layer with batch first parameter.\n",
        "- a Dropout layer of probability 0.8.\n",
        "- a linear classifier to make classifications.\n",
        "\n",
        "For the LSTM layer, as the documentation states, we need to instantiate an initial hidden state and an initial cell state. We will instantiate them to zero.\n",
        "\n",
        "You should reach 73-75% test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoFXQpwWBOd8"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Dropout, LSTM, Linear\n",
        "from torch import nn\n",
        "\n",
        "class LSTMModel(Module):\n",
        "    \"\"\"\n",
        "    LSTM model.\n",
        "\n",
        "    Args:\n",
        "      input_dim: the input dimension.\n",
        "      output_dim: the output dimension.\n",
        "      hidden_dim: the hidden dimension.\n",
        "      n_layers: number of RNN layers stacked.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int = 512, output_dim: int = 20, hidden_dim: int = 256, n_layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        # --- START CODE HERE (17)\n",
        "        # Instantiate the LSTM, dropout and linear layers.\n",
        "        self.lstm =\n",
        "        self.dropout =\n",
        "        self.fc =\n",
        "        # --- END CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # --- START CODE HERE (18)\n",
        "        # Initiliaze the hidden and cell state to zero tensors for the LSTM layer.\n",
        "        # Retrieve last hidden state to apply dropout and linear classifier on.\n",
        "        \n",
        "        # --- END CODE HERE\n",
        "        \n",
        "        return output\n",
        "    \n",
        "LSTMModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bilm6ml4CGy-"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Run over epochs\n",
        "nb_epoch = 50\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LSTMModel().to(device)\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "loss_fn = CrossEntropyLoss().to(device)\n",
        "\n",
        "train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n",
        "  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Rnw1qyaE4t"
      },
      "outputs": [],
      "source": [
        "display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDe99xH8DjhK"
      },
      "source": [
        "### GRU Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V9T7g5byMww"
      },
      "source": [
        "We will implement a GRU Model based on the pytorch [GRU layer](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) that is supposed to be better or equivalent to LSTM.\n",
        "\n",
        "![](https://blog.floydhub.com/content/images/2019/07/image14.jpg)\n",
        "\n",
        "We will make use of the following layers:\n",
        "- a GRU layer with batch first parameter.\n",
        "- a Dropout layer of probability 0.8.\n",
        "- a linear classifier to make classifications.\n",
        "\n",
        "For the GRU layer, as the documentation states, we need to instantiate an initial hidden state. We will instantiate it to zero.\n",
        "\n",
        "You should reach 75-77% test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJY3r8OGCJe7"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Dropout, Linear, Module, GRU\n",
        "from torch import nn\n",
        "\n",
        "class GRUModel(Module):\n",
        "    \"\"\"\n",
        "    GRU model.\n",
        "\n",
        "    Args:\n",
        "      input_dim: the input dimension.\n",
        "      output_dim: the output dimension.\n",
        "      hidden_dim: the hidden dimension.\n",
        "      n_layers: number of RNN layers stacked.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int = 512, output_dim: int = 20, hidden_dim: int = 512, n_layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # --- START CODE HERE (19)\n",
        "        # Instantiate the GRU, dropout and linear layers.\n",
        "        self.rnn = \n",
        "        self.dropout =\n",
        "        self.fc = \n",
        "        # --- END CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        batch_size = x.shape[0]        \n",
        "\n",
        "        # --- START CODE HERE (20)\n",
        "        # Initiliaze the hidden state to a zero tensor for the GRU layer.\n",
        "        # Retrieve last hidden state to apply dropout and linear classifier on.\n",
        "\n",
        "        # --- END CODE HERE\n",
        "        \n",
        "        return output\n",
        "\n",
        "RNNModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crvov3mgD64v"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Run over epochs\n",
        "nb_epoch = 50\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = GRUModel().to(device)\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "loss_fn = CrossEntropyLoss().to(device)\n",
        "\n",
        "train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n",
        "  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjeoLWLcaRp3"
      },
      "outputs": [],
      "source": [
        "display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOxuSzpfa_Hq"
      },
      "source": [
        "## BONUS\n",
        "\n",
        "To improve your results you can try various things:\n",
        "- Use another feature extractor:\n",
        "  - Resnet34, Resnet50, ...\n",
        "  - VGG.\n",
        "  - DenseNet.\n",
        "  - ...\n",
        "- Change hyper-parameters for RNN, LSTM, GRU layers:\n",
        "  - use several layers.\n",
        "  - try bidirectional parameter.\n",
        "  - increase the hidden dimension.\n",
        "- Change the optimizer parameters.\n",
        "- Use shorter or larger clip sequences.\n",
        "- Try to use 3D networks instead of Recurent networks such as ResNet3D , or S3D.\n",
        "- Try to use attention based networks.\n",
        "- ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbgG4NZ8a0gk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('machine_learning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d38c4cc43721ac0d114dc8b1e646949e6883c7f0d964ef46ebccf5a0ec111b05"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
