{"cells":[{"cell_type":"markdown","metadata":{"id":"gzYw0Wh_2MiX"},"source":["# TP7: Fine-tuning BERT on Q&A tasks\n","\n","**Authors:** \n","- julien.denize@centralesupelec.fr\n","- tom.dupuis@centralesupelec.fr\n","\n","\n","If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n","\n","For this tp you need to have some ground understanding of pytorch. A basic introduction is available [here](https://pytorch.org/tutorials/beginner/basics/intro.html).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ufFFeRxvuQpb"},"source":["## Objective\n","\n","In this TP, we will implement solutions for the Question & Answering (Q&A) task by Fine-tuning a pretrained distilbert.\n","\n","This TP is built on the [HuggingFace Q&A tutorial](https://huggingface.co/docs/transformers/tasks/question_answering), therefore it relies on libraries associated to HuggingFace. \n","\n","Question answering tasks return an answer given a question. There are two common forms of question answering:\n","- Extractive: extract the answer from the given context.\n","- Abstractive: generate an answer from the context that correctly answers the question.\n","\n","In this TP, we will show you how to fine-tune [Distilbert](https://huggingface.co/docs/transformers/model_doc/distilbert) on the SQuAD dataset for extractive question answering.\n","\n","Distilbert is a smaller transformer architecture than BERT that has been trained by [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation) of BERT to provide a lightweight faster NLP model with high performance.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WRUiEYYhwIBv"},"source":["## Your task\n","\n","Fill the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKtndvKm2Gx0"},"outputs":[],"source":["import torch\n","import numpy as np\n","import random\n","\n","# Seed everything\n","seed=42\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"vwqqRns9XFOZ"},"source":["## Install the required libraries"]},{"cell_type":"markdown","metadata":{"id":"KBeMTVkRYpKe"},"source":["We need to install the following pip packages:\n","- [datasets](https://pypi.org/project/datasets/): to load datasets available on the [HuggingFace Datasets Hub](https://huggingface.co/datasets). \n","- [transformers](https://pypi.org/project/transformers/):  to load thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio in Pytorch, Tensorflow or JAX."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lcl5Lf-gwN2v"},"outputs":[],"source":["! pip install datasets transformers"]},{"cell_type":"markdown","metadata":{"id":"4aB3DYYlX1zC"},"source":["## Load the dataset"]},{"cell_type":"markdown","metadata":{"id":"HF37_b3hYZm7"},"source":["We will use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/).\n","\n",">Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n","\n","We will instantiate the train and validation splits via the [load_dataset](https://huggingface.co/docs/datasets/v1.11.0/splits.html) function.\n","\n","There are 87.599 elements in the train split and 10.570 elements in the validation split. We will only request 1% of each to avoid long training time. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07h1xn1LW8bV"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# --- START CODE HERE (01)\n","# Load the SQUAD dataset with 1% of each different splits.\n","dataset =\n","\n","train_dataset =\n","validation_dataset =\n","# --- END CODE HERE\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"Utd6TS2ZimtC"},"source":["Now we can have access to the samples in the datasets.\n","\n","In each sample we have the following information:\n","- the id of the wikipedia article.\n","- the title of the wikipedia article.\n","- the context that contains the answer to the question.\n","- the question.\n","- the answer along with the index of where the answer start."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkLPSav8i6Dl"},"outputs":[],"source":["train_dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"QTv-Z_hwkFe3"},"source":["HuggingFace provides a nice function to better show what the data looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAM6FB43jjV5"},"outputs":[],"source":["from datasets import ClassLabel, Sequence\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n","            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n","    display(HTML(df.to_html()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YT3MOBm3kJtP"},"outputs":[],"source":["show_random_elements(train_dataset, 3)"]},{"cell_type":"markdown","metadata":{"id":"VvUhN8m7kW2p"},"source":["## Preprocess the training data\n"]},{"cell_type":"markdown","metadata":{"id":"es8h9C7pYDho"},"source":["\n","Now that we have access to the data, we need to preprocess it to feed it to our neural networks.\n","\n","In NLP, this step consist of making the tokenization of the data, meaning convert the string words into unique IDs.\n","\n","Our model requires the following as input:\n","- A first sequence that is the question.\n","- A separator token [SEP].\n","- A second sequence that may contain the answer.\n","\n","The label is given by the start and end indices of the tokens that compose the answer.\n","\n","![](https://miro.medium.com/max/1400/1*QhIXsDBEnANLXMA0yONxxA.png)"]},{"cell_type":"markdown","metadata":{"id":"NIReoeO1siVU"},"source":["### Instantiate the tokenizer\n","\n","We will use the [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto) class provided by HuggingFace as this will ensure we use the tokenizer that was used to train the distilbert model. For that, we need to use the right checkpoint of the model. The list of checkpoints is available [here](https://huggingface.co/models) and you need to retrieve the basic checkpoint for distilbert that do not care about the case (ENGLISH = english). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtR4hIIYp3v1"},"outputs":[],"source":["# --- START CODE HERE (02)\n","# Import the auto tokenizer class and instantiate it\n","\n","    \n","model_checkpoint =\n","tokenizer =\n","# --- END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"jJiwbTw_py8u"},"source":["You can try the tokenizer with custom strings or from our data. Tokenizer accepts tuple as input, but returns only one concatenated tokenized output with a separator token [SEP] with a starting token [CLS]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dftzIeKfrcrf"},"outputs":[],"source":["custom_string = \"Hi, I would love to test the tokenizer.\"\n","custom_string_2 = \"Sure, go ahead and verify that english = ENGLISH = ENglISh.\"\n","tokenized_custom_strings = tokenizer(custom_string, custom_string_2)\n","tokenized_custom_strings"]},{"cell_type":"markdown","metadata":{"id":"NbQZg1GHI2rW"},"source":["You can now decode the sequence and retrieve the initial string with the special tokens. You can also verify that the case is no longer present in the string as our model do not make the difference between lower and upper case. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGp30TPvwq8s"},"outputs":[],"source":["tokenizer.decode(tokenized_custom_strings[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"mT0leg9kJImx"},"source":["Here we can apply the tokenizer on one question from our training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKQnsoeYrjei"},"outputs":[],"source":["print(train_dataset[0][\"question\"])\n","print(tokenizer(train_dataset[0][\"question\"]))\n","print(tokenizer.decode(tokenizer(train_dataset[0][\"question\"])[\"input_ids\"]))"]},{"cell_type":"markdown","metadata":{"id":"Vk-jB-BeslM0"},"source":["### Deal with long contexts\n","\n","Our model can only take a maximum number of tokens per input. Our input is composed of both the question and the context separated by the special token [SEP]. \n","\n","However, in our dataset we might have some samples where the question plus the context length is larger than this maximum number of tokens. We cannot just truncate the input as for some other tasks as the answer to the question might be located in the cut part. \n","\n","Instead, a long context will be splitted in several input features, each of length shorter than the maximum length of the model. To avoid that the answer is located on the splitting point, we will make the input features overlap."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SPNp_LHsML_"},"outputs":[],"source":["max_length = 384 # The maximum length of a feature (question and context)\n","doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."]},{"cell_type":"markdown","metadata":{"id":"JMoDoYqPvk_z"},"source":["Below is the code to find the first example with a long input:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whWe6iccvkiA"},"outputs":[],"source":["for i, example in enumerate(train_dataset):\n","    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n","        long_context_idx = i\n","        break\n","long_example = train_dataset[long_context_idx]\n","long_example, len(tokenizer(long_example[\"question\"] + long_example[\"context\"])[\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"OVqBqbFlvikK"},"source":["To split the input in several features, we need to correctly configure our tokenizer and feed it with inputs following these requirements:\n","- Pass to the tokenizer the tuple of the question and the context. It will automatically add the [SEP] token between the two.\n","- Force the tokenizer to split the input if it is too large:\n","  - only the second part (the context) can be truncated so that the question is shared by all new inputs.\n","  - allow overlapping between tokens.\n","\n","All these requirements can be done thanks to the [tokenizer utilities](https://huggingface.co/docs/transformers/v4.23.1/en/internal/tokenization_utils) by applying the correct parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ixyM9XXyVvN"},"outputs":[],"source":["# --- START CODE HERE (03)\n","# Tokenize the long example with the requirements defined above.\n","tokenized_long_example = \n","# --- END CODE HERE\n","\n","print(f\"The long example now has {len(tokenized_long_example['input_ids'])} inputs with length {[len(x) for x in tokenized_long_example['input_ids']]}.\") # Should have 2 inputs with length [384, 157]\n","for sequence in tokenized_long_example['input_ids']:\n","  print(tokenizer.decode(sequence))"]},{"cell_type":"markdown","metadata":{"id":"ELPkQLQp4t1W"},"source":["The problem with the above solution is that we lack the information of where is located the answer: we need to know where is located the answer for each feature provided. \n","\n","The model require the start and end positions of the answers in the tokens, so we will also need to map parts of the original context to some tokens.\n","\n","We need for each index of our feature the corresponding start and end character in the original text that gave our token in the format (`start_char`, `end_char`). The first token (`[CLS]`) has (0, 0) because it is a special added token that was not present in the original sentence.\n","\n","This can be done using the tokenizer utilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3xJF4kMzgZL"},"outputs":[],"source":["# --- START CODE HERE (04)\n","# Tokenize the long example with the new requirement defined above.\n","tokenized_long_example = \n","# --- END CODE HERE\n","\n","print(f\"The long example now has {len(tokenized_long_example['input_ids'])} inputs with length {[len(x) for x in tokenized_long_example['input_ids']]}.\") # Should have 2 inputs with length [384, 157]\n","for sequence, mapping in zip(tokenized_long_example['input_ids'], tokenized_long_example[\"offset_mapping\"]):\n","  print(\"\\n\")\n","  print(tokenizer.decode(sequence))\n","  print(sequence)\n","  print(mapping)"]},{"cell_type":"markdown","metadata":{"id":"Pyg8Uppx-wBW"},"source":["The mapping can be used to find the position of the start and end tokens of our answer in a feature. To avoid the question part we can use the `sequence_ids` field provided by the tokenizer output to have the knowledge of which tokens are part of the first sequence (the question) or the second sequence (the context, or part of the context).\n","\n","It returns for each token, the sequence ID (0 for question, 1 for context) and None for special tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5ZC9JbUzipM"},"outputs":[],"source":["sequence_ids = tokenized_long_example.sequence_ids()\n","print(sequence_ids)"]},{"cell_type":"markdown","metadata":{"id":"mGOjgQeG_kAB"},"source":["Now, we can retrieve the answer from our features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmlFl0ut_mhU"},"outputs":[],"source":["answer = long_example[\"answers\"] # Retrieve the answer from the example\n","start_char = answer[\"answer_start\"][0] # Retrieve the index of the start character of the answer\n","end_char = start_char + len(answer[\"text\"][0]) # Retrieve the index of the end character of the answer\n","\n","# Iterate over the features\n","for i in range(len(tokenized_long_example[\"input_ids\"])):\n","  print(f\"Looking for the answer `{answer['text'][0]}` to the question `{long_example['question']}` in feature {i+1}.\")\n","  print(f\"The feature contains the following decoded sequence:\\n{tokenizer.decode(tokenized_long_example['input_ids'][i])}\")\n","  \n","  # Start token index of the current span in the text.\n","  token_start_index = 0\n","\n","  # --- START CODE HERE (05)\n","  # Find where the context sequence starts and store it in the variable token_start_index.\n","  while :\n","\n","  # --- END CODE HERE\n","\n","  # --- START CODE HERE (06)\n","  # Find where the context sequence ends and store it in the variable token_end_index.\n","  token_end_index =\n","  while:\n","\n","  # --- END CODE HERE\n","\n","  offsets = tokenized_long_example[\"offset_mapping\"][i]\n","  # Detect if the answer is out of the span.\n","  if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","\n","      # --- START CODE HERE (07)\n","      # Find where are the start_position and end_position of the answer.\n","      # Move the token_start_index and token_end_index to the two ends of the answer.\n","      while:\n","          token_start_index += 1\n","      start_position = token_start_index - 1\n","      while :\n","          token_end_index -= 1\n","      end_position = token_end_index + 1\n","      # --- END CODE HERE\n","\n","      print(f\"Answer found by the tokenizer at the token positions: {start_position}, {end_position}\")\n","      print(f\"{tokenizer.decode(tokenized_long_example['input_ids'][i][start_position: end_position+1])}\")\n","  else:\n","      print(\"The answer is not in this feature.\")\n","  print(\"\\n\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DSpLL3KUXsnr"},"source":["### Tokenize the whole dataset\n","\n","Now we can implement a function that will prepare the whole dataset following the above process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2GyeNtAIt5j"},"outputs":[],"source":["def prepare_train_features(examples, tokenizer, max_length: int = 384, doc_stride: int = 128):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n","\n","    # For this notebook to work with any kind of models, we need to account for the special case where the model\n","    # expects padding on the left (in which case we switch the order of the question and the context)\n","    pad_on_right = tokenizer.padding_side == \"right\"\n","\n","    # --- START CODE HERE (08)\n","    # Apply the tokenizer as before except that be careful to correctly setup the order of question and context \n","    # given the value of the boolean pad_on_right.\n","    tokenized_examples =\n","    # --- END CODE HERE\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    # The offset mappings will give us a map from token to character position in the original context. This will\n","    # help us compute the start_positions and end_positions.\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","    # Let's label those examples!\n","    tokenized_examples[\"start_positions\"] = []\n","    tokenized_examples[\"end_positions\"] = []\n","\n","    # Iterate over the offset mapping from the features.\n","    for i, offsets in enumerate(offset_mapping):\n","        # We will label impossible answers with the index of the CLS token.\n","        input_ids = tokenized_examples[\"input_ids\"][i]\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        answers = examples[\"answers\"][sample_index]\n","\n","        # --- START CODE HERE (09)\n","        # If no answers are given, set the cls_index as answer.\n","        if :\n","\n","        # --- END CODE HERE\n","        else:\n","            # Start/end character index of the answer in the text.\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","           \n","            # --- START CODE HERE (10)\n","            # Find where the context sequence starts and ends as before. Be careful about the pad_on_right boolean.\n","            while :\n","                token_start_index += 1\n","\n","            token_end_index =\n","            while :\n","                token_end_index -= 1\n","            # --- END CODE HERE\n","\n","            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                # --- START CODE HERE (11)\n","                # Label impossible answers with the index of the CLS token.\n","                tokenized_examples[\"start_positions\"].\n","                tokenized_examples[\"end_positions\"].\n","                # --- END CODE HERE\n","            else:\n","                # --- START CODE HERE (12)\n","                # Find where are the start_position and end_position of the answer as before.\n","                while :\n","                    token_start_index += 1\n","                tokenized_examples[\"start_positions\"].\n","                while :\n","                    token_end_index -= 1\n","                tokenized_examples[\"end_positions\"].\n","                # --- END CODE HERE\n","            \n","\n","    return tokenized_examples"]},{"cell_type":"markdown","metadata":{"id":"N0jtaWMDE05A"},"source":["This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp3_0N3ML-Cq"},"outputs":[],"source":["features = prepare_train_features(train_dataset[:5], tokenizer)\n","len(features[\"input_ids\"]), len(features[\"input_ids\"][0]) # should return (5, 384)"]},{"cell_type":"markdown","metadata":{"id":"C5wWMMHGMVYe"},"source":["Now, we can apply this function to our dataset using the [`.map`](https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.Dataset.map) operator from datasets to apply this tokenization process on our whole training dataset. \n","\n","We will apply the same function to the validation dataset to evaluate our model during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdsIPpw_MSpv"},"outputs":[],"source":["num_indices_to_keep_data = round(0.01 * len(train_dataset)) # 1% of data to keep.\n","indices = np.random.choice(range(len(train_dataset)), num_indices_to_keep_data, replace=False)\n","print(len(indices))\n","print(type(train_dataset))\n","subsample_train_dataset = train_dataset[indices]\n","\n","# --- START CODE HERE (13)\n","# Apply the prepare_train_features to the train_dataset and validation_dataset. Provide the tokenizer to the function and batch the data.\n","# Finally, remove the column names from the dataset.\n","tokenized_train_dataset = \n","tokenized_validation_dataset = \n","# --- END CODE HERE\n","tokenized_train_dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"YTiGQsdxNw5l"},"source":["## Fine-tune the model"]},{"cell_type":"markdown","source":["Now that we have transformed the dataset to feed the model, we will instantiate our model and train it.\n","\n","\n","First, we will retrieve the model thanks to the [auto model for question answering](https://huggingface.co/docs/transformers/model_doc/auto) from HuggingFace."],"metadata":{"id":"8RuRor-hAZPh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jjt4VZkENzck"},"outputs":[],"source":["# --- START CODE HERE (14)\n","# Import the correct auto model class and instantiate the model.\n","\n","model = \n","# --- END CODE HERE"]},{"cell_type":"markdown","source":["To train a model, HuggingFace expects to instantiate a [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) taking as parameters:\n","- the model\n","- the arguments to configure the trainer\n","- the train dataset\n","- the eval dataset\n","- the default [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator)\n","- the tokenizer\n","\n","First we will instantiate a [TrainerArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) with the following parameters:\n","- evaluation at each epoch\n","- learning rate of value 2e-5\n","- batch size of 16 to train\n","- batch size of 16 to evaluate\n","- train for 5 epochs\n","- weight decay of 0.01"],"metadata":{"id":"b0v9PHLyBFyC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UUDDQbhOOHJ"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","model_name = model_checkpoint.split(\"/\")[-1]\n","\n","# --- START CODE HERE (14)\n","# Instantiate the Training Arguments.\n","args = \n","# --- END CODE HERE"]},{"cell_type":"markdown","source":["Now that we have the training arguments and the model, we can instantiate the trainer."],"metadata":{"id":"6eLFWUHGCSet"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZeTiBoJOU2K"},"outputs":[],"source":["# --- START CODE HERE (15)\n","# Import the trainer and the data collator.\n","\n","# --- END CODE HERE\n","\n","# --- START CODE HERE (16)\n","# Instantiate the Trainer.\n","trainer = \n","# --- END CODE HERE"]},{"cell_type":"markdown","source":["Finally, we can launch the training that should last around 2 minutes to train."],"metadata":{"id":"WmCilHiDAB6F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXXtBCixPofm"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"lpsRi1MwWecW"},"source":["### Evaluate our model"]},{"cell_type":"markdown","source":["After training our model, we can start evaluating it.\n","\n","For that we need to retrieve the prediction of our model. The following code gives us the keys returned by our model for a validation batch."],"metadata":{"id":"QpJp5HCobtrd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZYiHiyOWhVy"},"outputs":[],"source":["import torch\n","\n","for batch in trainer.get_eval_dataloader():\n","    break\n","batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n","with torch.no_grad():\n","    output = trainer.model(**batch)\n","output.keys()"]},{"cell_type":"markdown","source":["Our model predicts two probability distributions over the tokens:\n","- the start token probability called the `start_logits`.\n","- the end token probability called the `end_logits`."],"metadata":{"id":"aYAwfmrWb8tC"}},{"cell_type":"code","source":["output.start_logits.shape, output.end_logits.shape"],"metadata":{"id":"JvL-y0NUDJf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To output the actual hard prediction we take the argmax of each distribution. We can observe several issues:\n","- sometimes the end token predicted is before the start token which is impossible.\n","- the predicted token could be inside the question.\n","- if our context is too large, we will have several predictions for each feature provided by the tokenizer.\n","\n","Therefore, we need a procedure to select the best predictions."],"metadata":{"id":"_9-uR0wxcWgL"}},{"cell_type":"code","source":["output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"],"metadata":{"id":"ND8gfxVKDLTE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following cell, we will make our logits follow this pipeline:\n","- keep the 20 best propositions for each `start_logits` and each `end_logits` (the maximum value of the probability distributions).\n","- make pair values of each `start_logits` and `end_logits` if the `end_logits` index is after the `start_logits` index.\n","\n","The idea is that every token proposition for both start and end should be taken into account and not only paired predictions especially when the first best paired predictions are not possible."],"metadata":{"id":"-AMGYKAodENU"}},{"cell_type":"code","source":["import numpy as np\n","\n","n_best_size = 20\n","start_logits = output.start_logits[0].cpu().numpy()\n","end_logits = output.end_logits[0].cpu().numpy()\n","\n","# --- START CODE HERE (17)\n","# Only keep the best propositions index.\n","start_indexes = \n","end_indexes = \n","# --- END CODE HERE\n","valid_answers = []\n","for start_index in start_indexes:\n","    for end_index in end_indexes:\n","        # --- START CODE HERE (18)\n","        # Only keep the valid pairs.\n","        if :\n","        # --- END CODE HERE\n","            valid_answers.append(\n","                {\n","                    \"score\": start_logits[start_index] + end_logits[end_index],\n","                    \"text\": \"\" # Later we will find a way to get back the original substring corresponding to the answer in the context\n","                }\n","            )\n","print(f\"We kept only {len(valid_answers)} valid pairs from {len(start_indexes) * len(end_indexes)} best pair propositions from {sum(start_logits.shape) * sum(end_logits.shape)} possible pairs.\")"],"metadata":{"id":"EOjrwZniDOMO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To retrieve all validation features, we need to add two things to our validation pipeline:\n","- verify that our pairs are inside the context and not the question.\n","- retrieve the actual text for the model instead of the tokens.\n","\n","We need to tokenize all our validation data. We will implement a process pipeline slightly different from `prepare_train_features` that we implemented before."],"metadata":{"id":"zmFzg4-Ufh22"}},{"cell_type":"code","source":["def prepare_validation_features(examples):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n","\n","    # For this notebook to work with any kind of models, we need to account for the special case where the model\n","    # expects padding on the left (in which case we switch the order of the question and the context)\n","    pad_on_right = tokenizer.padding_side == \"right\"\n","\n","    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\" if pad_on_right else \"context\"],\n","        examples[\"context\" if pad_on_right else \"question\"],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # --- START CODE HERE (19)\n","    # Apply the tokenizer as for prepare_train_features\n","    tokenized_examples = \n","    # --- END CODE HERE\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","\n","    # We keep the example_id that gave us this feature and we will store the offset mappings.\n","    tokenized_examples[\"example_id\"] = []\n","\n","    for i in range(len(tokenized_examples[\"input_ids\"])):\n","        # --- START CODE HERE (20)\n","        # Grab the text sequence corresponding to that feature.\n","        sequence_ids = \n","        # --- END CODE HERE\n","\n","        context_index = 1 if pad_on_right else 0\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n","\n","        # --- START CODE HERE (21)\n","        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n","        # position is part of the context or not.\n","        tokenized_examples[\"offset_mapping\"][i] = \n","        # --- END CODE HERE\n","\n","\n","    return tokenized_examples"],"metadata":{"id":"kL39lTERDQFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can apply this function to our dataset using the [`.map`](https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.Dataset.map) operator from datasets to apply this tokenization process on our whole validation dataset. "],"metadata":{"id":"HKUbt2C2-KUv"}},{"cell_type":"code","source":["# --- START CODE HERE (22)\n","# Apply the prepare_validation_features to the validation_dataset. Provide the tokenizer to the function and batch the data.\n","# Finally, remove the column names from the dataset.\n","validation_features = \n","# --- END CODE HERE"],"metadata":{"id":"-KkBlNL-DRbB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the `validation_features`, we will make predictions thanks to the [trainer](https://huggingface.co/docs/transformers/main_classes/trainer)."],"metadata":{"id":"SkdVBZv8-RYz"}},{"cell_type":"code","source":["raw_predictions = trainer.predict(validation_features)"],"metadata":{"id":"10dT73aXDRqo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"],"metadata":{"id":"2MKPUMtz-smj"}},{"cell_type":"code","source":["validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"],"metadata":{"id":"8OXr2bXgDS4q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To refine our validation pipeline and eliminate irrelevant answers we will filter:\n","- the answers containing `None` in the offset mappings as it corresponds to a part of the question\n","- the answers longer than the hyper-parameter `max_answer_length`"],"metadata":{"id":"tvB5Ny6Q-7Kk"}},{"cell_type":"code","source":["max_answer_length = 30\n","start_logits = output.start_logits[0].cpu().numpy()\n","end_logits = output.end_logits[0].cpu().numpy()\n","offset_mapping = validation_features[0][\"offset_mapping\"]\n","\n","# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n","# an example index\n","context = validation_dataset[0][\"context\"]\n","\n","# --- START CODE HERE (23)\n","# Only keep the best propositions index as before.\n","start_indexes = \n","end_indexes = \n","# --- END CODE HERE\n","valid_answers = []\n","for start_index in start_indexes:\n","    for end_index in end_indexes:\n","        # --- START CODE HERE (24)\n","        # Filter out-of-scope answers: indices out of bounds or in the question.\n","        if :\n","        # --- END CODE HERE\n","            continue\n","\n","        # --- START CODE HERE (25)\n","        # Consider answers that are valid and shorter than max_answer_length.\n","        if :\n","        # --- END CODE HERE\n","            continue\n","\n","        start_char = offset_mapping[start_index][0]\n","        end_char = offset_mapping[end_index][1]\n","        valid_answers.append(\n","            {\n","                \"score\": start_logits[start_index] + end_logits[end_index],\n","                \"text\": context[start_char: end_char]\n","            }\n","        )\n","\n","valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n","valid_answers"],"metadata":{"id":"owTrlO2ODWwm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can compare to the actual ground-truth answer:"],"metadata":{"id":"42S5EeXRAsMN"}},{"cell_type":"code","source":["validation_dataset[0][\"answers\"]"],"metadata":{"id":"BLARf1g6DYSw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As mentioned in the code above, this was easy on the first feature because we knew it comes from the first example.\n","\n","For the other features, we will map between examples and their corresponding features. Since one example can give several features, we will gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"],"metadata":{"id":"awMpb9o6A242"}},{"cell_type":"code","source":["import collections\n","\n","features = validation_features\n","\n","example_id_to_index = {k: i for i, k in enumerate(validation_dataset[\"id\"])}\n","features_per_example = collections.defaultdict(list)\n","for i, feature in enumerate(features):\n","    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"],"metadata":{"id":"9l9GtlcZDZVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All combined together, this gives us this post-processing function:"],"metadata":{"id":"ADFQzsTwBDCs"}},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","\n","def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n","    all_start_logits, all_end_logits = raw_predictions\n","    # Build a map example to its corresponding features.\n","    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","    features_per_example = collections.defaultdict(list)\n","    for i, feature in enumerate(features):\n","        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","    # The dictionaries we have to fill.\n","    predictions = collections.OrderedDict()\n","\n","    # Logging.\n","    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","    # Let's loop over all the examples!\n","    for example_index, example in enumerate(tqdm(examples)):\n","        # Those are the indices of the features associated to the current example.\n","        feature_indices = features_per_example[example_index]\n","\n","        min_null_score = None # Only used if squad_v2 is True.\n","        valid_answers = []\n","        \n","        context = example[\"context\"]\n","        # Looping through all the features associated to the current example.\n","        for feature_index in feature_indices:\n","            # We grab the predictions of the model for this feature.\n","            start_logits = all_start_logits[feature_index]\n","            end_logits = all_end_logits[feature_index]\n","            # This is what will allow us to map some the positions in our logits to span of texts in the original\n","            # context.\n","            offset_mapping = features[feature_index][\"offset_mapping\"]\n","\n","            # Update minimum null prediction.\n","            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","            if min_null_score is None or min_null_score < feature_null_score:\n","                min_null_score = feature_null_score\n","\n","            # --- START CODE HERE (26)\n","            # Only keep the best propositions index as before.\n","            start_indexes = \n","            end_indexes = \n","            # --- END CODE HERE\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # --- START CODE HERE (27)\n","                    # Filter out-of-scope answers: indices out of bounds or in the question as before.\n","                    if :\n","                    # --- END CODE HERE\n","                        continue\n","                    # --- START CODE HERE (28)\n","                    # Consider valid answers and the ones that have length shorter than max_answer_length.\n","                    if :\n","                    # --- END CODE HERE\n","                        continue\n","\n","                    start_char = offset_mapping[start_index][0]\n","                    end_char = offset_mapping[end_index][1]\n","                    valid_answers.append(\n","                        {\n","                            \"score\": start_logits[start_index] + end_logits[end_index],\n","                            \"text\": context[start_char: end_char]\n","                        }\n","                    )\n","        \n","        if len(valid_answers) > 0:\n","            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","        else:\n","            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n","            # failure.\n","            best_answer = {\"text\": \"\", \"score\": 0.0}\n","        \n","        # Let's pick our final answer: the best one\n","        predictions[example[\"id\"]] = best_answer[\"text\"]\n","\n","    return predictions"],"metadata":{"id":"9M-aoJgmDa9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can apply the post-processing function to our predictions:"],"metadata":{"id":"67yWNaXhCPTN"}},{"cell_type":"code","source":["# --- START CODE HERE (29)\n","# Apply our postprocess_qa_predictions to our predictions.\n","final_predictions = \n","# --- END CODE HERE"],"metadata":{"id":"uCZtOtc6DcaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we can load the metric from the datasets library."],"metadata":{"id":"EW278jRADHJg"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"squad\")"],"metadata":{"id":"ak48Ci_CDdk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n","references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in validation_dataset]\n","metric.compute(predictions=formatted_predictions, references=references)"],"metadata":{"id":"0QNRI8RQDepx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can improve this result by having a larger dataset or train longer !"],"metadata":{"id":"S9nqJbZ2CIwG"}},{"cell_type":"markdown","metadata":{"id":"MLJvhtkY1gQT"},"source":["## What to do now ?\n","\n","If you want, you can lookup for datasets in your own language and see if distilbert performs correctly. Generally, a model that was learnt on the same language as your dataset will work better than a general model that was learnt on several languages or, obviously, on a totally different language. \n","\n","For example for french, Camembert is a BERT model but trained on french datasets and obtain a very good performance for french NLP tasks.\n","\n","You can take a look at other HuggingFace tutorial that cover other tasks to see what is the tokenization process, how the model is different for such tasks:\n","- [translation](https://huggingface.co/docs/transformers/tasks/translation)\n","- [summarization](https://huggingface.co/docs/transformers/tasks/summarization)\n","- ...\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQN_3Hp53KLY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1C-uFUwuxdln1E2bnZE25jULSXCNYQ1cA","timestamp":1666298632949}],"collapsed_sections":[],"authorship_tag":"ABX9TyNr6X9zIKPX2i8IkzN7y1Pd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}