{"cells":[{"cell_type":"markdown","metadata":{"id":"yuvR0L1jC447"},"source":["# TP2: Convolutional Neural Networks in Pytorch\n","\n","**Authors:** \n","- julien.denize@cea.fr\n","- tom.dupuis@cea.fr\n","\n","\n","If you have questions or suggestions, contact us and we will gladly answer and take into account your remarks.\n","\n","For this tp you need to have some ground understanding of pytorch and basics introduction. It is available [here](https://pytorch.org/tutorials/beginner/basics/intro.html).\n","\n","\n","## Objective\n","\n","In this TP, we will implement Convolutional Neural Networks (CNNs) to classify correctly images for the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n","\n","The CIFAR-10 dataset consists of 60 000 32x32 colour images in 10 classes, with 6 000 images per class. There are 50 000 training images and 10 000 test images.\n","\n","We will first design a custom made CNN using Pytorch to make the classification and then use an architecture available from torchvision that we will either train from scratch or finetune.\n","\n","We will compare the accuracy on the test set for all networks.\n","\n","Finally, we will also make use of Data Augmentation to further improve the generalization of our custom model to the testing data.\n","\n","Follow the TP sequentially, otherwise you could spoil yourself answers of the first part :) \n","\n","## Your task\n","\n","Fill the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)\n","\n","## Custom Made CNN\n","\n","Our networks will be implemented as follows:\n","\n","  - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n","  - a ReLu activation function\n","\n","  - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n","  - a ReLu activation function\n","  - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n","\n","  - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n","  - a ReLu activation function\n","  - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n","\n","  - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n","\n","  - a Fully-Connected layer of output size 10\n","\n","\n","We will optimize it using the SGD optimizer with $lr = 0.01$, $momentum = 0.9$ and the cross-entropy loss.\n","\n","In this TP, in order to speed-up computations, we will use the GPU, so do not forget [to put on GPU](https://pytorch.org/docs/stable/notes/cuda.html) every objects required, we will remind you when it is needed.\n"]},{"cell_type":"markdown","metadata":{"id":"zsSwzoIQfVGJ"},"source":["### Load dataset\n","\n","To load the dataset, refer to the [pytorch documentation](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html).\n","\n","You will need to pass a [transform](https://pytorch.org/vision/stable/transforms.html) to the dataset to perform these operations:\n","- Transform the image to Tensor\n","- Normalize the data according to CIFAR10 statistics\n","\n","These transforms are available from the link above.\n","\n","You will also need a train and a test [dataloaders](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to iterate over the training and testing datasets. To configure the dataloader here is what to know:\n","- We will use a batch size of 128.\n","- Only training data needs to be shuffled to avoid learning the order of data.\n","- Colab has two cpus so the number of workers needs to be inferior or equal to 2.\n","- Drop the last batch only for training data.\n","\n","Because Colab provides us few resources, we will also downsample our datasets to perform quicker experiments and not just wait for completion of cells."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["['AVG',\n"," 'AggregationType',\n"," 'AliasDb',\n"," 'AnyType',\n"," 'Argument',\n"," 'ArgumentSpec',\n"," 'BFloat16Storage',\n"," 'BFloat16Tensor',\n"," 'BenchmarkConfig',\n"," 'BenchmarkExecutionStats',\n"," 'Block',\n"," 'BoolStorage',\n"," 'BoolTensor',\n"," 'BoolType',\n"," 'BufferDict',\n"," 'ByteStorage',\n"," 'ByteTensor',\n"," 'CONV_BN_FUSION',\n"," 'CallStack',\n"," 'Callable',\n"," 'Capsule',\n"," 'CharStorage',\n"," 'CharTensor',\n"," 'ClassType',\n"," 'Code',\n"," 'CompilationUnit',\n"," 'CompleteArgumentSpec',\n"," 'ComplexDoubleStorage',\n"," 'ComplexFloatStorage',\n"," 'ComplexType',\n"," 'ConcreteModuleType',\n"," 'ConcreteModuleTypeBuilder',\n"," 'DeepCopyMemoTable',\n"," 'DeserializationStorageContext',\n"," 'DeviceObjType',\n"," 'DictType',\n"," 'DisableTorchFunction',\n"," 'DoubleStorage',\n"," 'DoubleTensor',\n"," 'EnumType',\n"," 'ErrorReport',\n"," 'ExecutionPlan',\n"," 'FUSE_ADD_RELU',\n"," 'FatalError',\n"," 'FileCheck',\n"," 'FloatStorage',\n"," 'FloatTensor',\n"," 'FloatType',\n"," 'FunctionSchema',\n"," 'Future',\n"," 'FutureType',\n"," 'Generator',\n"," 'Gradient',\n"," 'Graph',\n"," 'GraphExecutorState',\n"," 'HOIST_CONV_PACKED_PARAMS',\n"," 'HalfStorage',\n"," 'HalfTensor',\n"," 'INSERT_FOLD_PREPACK_OPS',\n"," 'IODescriptor',\n"," 'InferredType',\n"," 'IntStorage',\n"," 'IntTensor',\n"," 'IntType',\n"," 'InterfaceType',\n"," 'JITException',\n"," 'ListType',\n"," 'LiteScriptModule',\n"," 'LockingLogger',\n"," 'LoggerBase',\n"," 'LongStorage',\n"," 'LongTensor',\n"," 'MobileOptimizerType',\n"," 'ModuleDict',\n"," 'Node',\n"," 'NoneType',\n"," 'NoopLogger',\n"," 'NumberType',\n"," 'OperatorInfo',\n"," 'OptionalType',\n"," 'PRIVATE_OPS',\n"," 'ParameterDict',\n"," 'PyObjectType',\n"," 'PyTorchFileReader',\n"," 'PyTorchFileWriter',\n"," 'QInt32Storage',\n"," 'QInt8Storage',\n"," 'QUInt2x4Storage',\n"," 'QUInt4x2Storage',\n"," 'QUInt8Storage',\n"," 'REMOVE_DROPOUT',\n"," 'RRefType',\n"," 'SUM',\n"," 'ScriptClass',\n"," 'ScriptClassFunction',\n"," 'ScriptDict',\n"," 'ScriptDictIterator',\n"," 'ScriptDictKeyIterator',\n"," 'ScriptFunction',\n"," 'ScriptList',\n"," 'ScriptListIterator',\n"," 'ScriptMethod',\n"," 'ScriptModule',\n"," 'ScriptModuleSerializer',\n"," 'ScriptObject',\n"," 'ScriptObjectProperty',\n"," 'SerializationStorageContext',\n"," 'Set',\n"," 'ShortStorage',\n"," 'ShortTensor',\n"," 'Size',\n"," 'StaticModule',\n"," 'Storage',\n"," 'StorageBase',\n"," 'Stream',\n"," 'StreamObjType',\n"," 'StringType',\n"," 'SymIntType',\n"," 'TYPE_CHECKING',\n"," 'Tensor',\n"," 'TensorType',\n"," 'ThroughputBenchmark',\n"," 'TracingState',\n"," 'TupleType',\n"," 'Type',\n"," 'USE_GLOBAL_DEPS',\n"," 'USE_RTLD_GLOBAL_WITH_LIBTORCH',\n"," 'Union',\n"," 'UnionType',\n"," 'Use',\n"," 'Value',\n"," '_C',\n"," '_TypedStorage',\n"," '_UntypedStorage',\n"," '_VF',\n"," '__all__',\n"," '__annotations__',\n"," '__builtins__',\n"," '__cached__',\n"," '__config__',\n"," '__doc__',\n"," '__file__',\n"," '__future__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '__version__',\n"," '_adaptive_avg_pool2d',\n"," '_adaptive_avg_pool3d',\n"," '_add_batch_dim',\n"," '_add_relu',\n"," '_add_relu_',\n"," '_addmm_activation',\n"," '_aminmax',\n"," '_amp_foreach_non_finite_check_and_unscale_',\n"," '_amp_update_scale_',\n"," '_assert',\n"," '_assert_async',\n"," '_batch_norm_impl_index',\n"," '_cast_Byte',\n"," '_cast_Char',\n"," '_cast_Double',\n"," '_cast_Float',\n"," '_cast_Half',\n"," '_cast_Int',\n"," '_cast_Long',\n"," '_cast_Short',\n"," '_choose_qparams_per_tensor',\n"," '_classes',\n"," '_coalesce',\n"," '_compute_linear_combination',\n"," '_conj',\n"," '_conj_copy',\n"," '_conj_physical',\n"," '_convert_indices_from_coo_to_csr',\n"," '_convert_indices_from_csr_to_coo',\n"," '_convolution',\n"," '_convolution_mode',\n"," '_copy_from',\n"," '_copy_from_and_resize',\n"," '_ctc_loss',\n"," '_cudnn_ctc_loss',\n"," '_cudnn_init_dropout_state',\n"," '_cudnn_rnn',\n"," '_cudnn_rnn_flatten_weight',\n"," '_cufft_clear_plan_cache',\n"," '_cufft_get_plan_cache_max_size',\n"," '_cufft_get_plan_cache_size',\n"," '_cufft_set_plan_cache_max_size',\n"," '_cummax_helper',\n"," '_cummin_helper',\n"," '_debug_has_internal_overlap',\n"," '_det_lu_based_helper',\n"," '_det_lu_based_helper_backward_helper',\n"," '_dim_arange',\n"," '_dirichlet_grad',\n"," '_disable_functionalization',\n"," '_efficientzerotensor',\n"," '_embedding_bag',\n"," '_embedding_bag_forward_only',\n"," '_empty_affine_quantized',\n"," '_empty_per_channel_affine_quantized',\n"," '_enable_functionalization',\n"," '_euclidean_dist',\n"," '_fake_quantize_learnable_per_channel_affine',\n"," '_fake_quantize_learnable_per_tensor_affine',\n"," '_fake_quantize_per_tensor_affine_cachemask_tensor_qparams',\n"," '_fft_c2c',\n"," '_fft_c2r',\n"," '_fft_r2c',\n"," '_foreach_abs',\n"," '_foreach_abs_',\n"," '_foreach_acos',\n"," '_foreach_acos_',\n"," '_foreach_add',\n"," '_foreach_add_',\n"," '_foreach_addcdiv',\n"," '_foreach_addcdiv_',\n"," '_foreach_addcmul',\n"," '_foreach_addcmul_',\n"," '_foreach_asin',\n"," '_foreach_asin_',\n"," '_foreach_atan',\n"," '_foreach_atan_',\n"," '_foreach_ceil',\n"," '_foreach_ceil_',\n"," '_foreach_cos',\n"," '_foreach_cos_',\n"," '_foreach_cosh',\n"," '_foreach_cosh_',\n"," '_foreach_div',\n"," '_foreach_div_',\n"," '_foreach_erf',\n"," '_foreach_erf_',\n"," '_foreach_erfc',\n"," '_foreach_erfc_',\n"," '_foreach_exp',\n"," '_foreach_exp_',\n"," '_foreach_expm1',\n"," '_foreach_expm1_',\n"," '_foreach_floor',\n"," '_foreach_floor_',\n"," '_foreach_frac',\n"," '_foreach_frac_',\n"," '_foreach_lgamma',\n"," '_foreach_lgamma_',\n"," '_foreach_log',\n"," '_foreach_log10',\n"," '_foreach_log10_',\n"," '_foreach_log1p',\n"," '_foreach_log1p_',\n"," '_foreach_log2',\n"," '_foreach_log2_',\n"," '_foreach_log_',\n"," '_foreach_maximum',\n"," '_foreach_minimum',\n"," '_foreach_mul',\n"," '_foreach_mul_',\n"," '_foreach_neg',\n"," '_foreach_neg_',\n"," '_foreach_norm',\n"," '_foreach_reciprocal',\n"," '_foreach_reciprocal_',\n"," '_foreach_round',\n"," '_foreach_round_',\n"," '_foreach_sigmoid',\n"," '_foreach_sigmoid_',\n"," '_foreach_sin',\n"," '_foreach_sin_',\n"," '_foreach_sinh',\n"," '_foreach_sinh_',\n"," '_foreach_sqrt',\n"," '_foreach_sqrt_',\n"," '_foreach_sub',\n"," '_foreach_sub_',\n"," '_foreach_tan',\n"," '_foreach_tan_',\n"," '_foreach_tanh',\n"," '_foreach_tanh_',\n"," '_foreach_trunc',\n"," '_foreach_trunc_',\n"," '_foreach_zero_',\n"," '_from_functional_tensor',\n"," '_fused_dropout',\n"," '_fused_moving_avg_obs_fq_helper',\n"," '_fw_primal_copy',\n"," '_grid_sampler_2d_cpu_fallback',\n"," '_has_compatible_shallow_copy_type',\n"," '_histogramdd_bin_edges',\n"," '_histogramdd_from_bin_cts',\n"," '_histogramdd_from_bin_tensors',\n"," '_import_dotted_name',\n"," '_index_put_impl_',\n"," '_indices_copy',\n"," '_initExtension',\n"," '_is_functional_tensor',\n"," '_is_zerotensor',\n"," '_jit_internal',\n"," '_linalg_check_errors',\n"," '_linalg_inv_out_helper_',\n"," '_linalg_qr_helper',\n"," '_linalg_svd',\n"," '_linalg_utils',\n"," '_load_global_deps',\n"," '_lobpcg',\n"," '_log_softmax',\n"," '_log_softmax_backward_data',\n"," '_logcumsumexp',\n"," '_lowrank',\n"," '_lstm_mps',\n"," '_lu_with_info',\n"," '_make_dual',\n"," '_make_dual_copy',\n"," '_make_per_channel_quantized_tensor',\n"," '_make_per_tensor_quantized_tensor',\n"," '_masked',\n"," '_masked_scale',\n"," '_masked_softmax',\n"," '_meta_registrations',\n"," '_mkldnn',\n"," '_mkldnn_reshape',\n"," '_mkldnn_transpose',\n"," '_mkldnn_transpose_',\n"," '_mps_convolution',\n"," '_mps_convolution_transpose',\n"," '_mps_linear_backward_weights',\n"," '_mps_max_pool2d',\n"," '_namedtensor_internals',\n"," '_native_multi_head_attention',\n"," '_neg_view',\n"," '_neg_view_copy',\n"," '_nested_from_padded',\n"," '_nested_from_padded_and_nested_example',\n"," '_nested_tensor_from_mask',\n"," '_nnpack_available',\n"," '_nnpack_spatial_convolution',\n"," '_ops',\n"," '_pack_padded_sequence',\n"," '_pad_packed_sequence',\n"," '_pin_memory',\n"," '_prims',\n"," '_register_device_module',\n"," '_remove_batch_dim',\n"," '_reshape_alias_copy',\n"," '_reshape_from_tensor',\n"," '_resize_output_',\n"," '_rowwise_prune',\n"," '_sample_dirichlet',\n"," '_saturate_weight_to_fp16',\n"," '_shape_as_tensor',\n"," '_six',\n"," '_sobol_engine_draw',\n"," '_sobol_engine_ff_',\n"," '_sobol_engine_initialize_state_',\n"," '_sobol_engine_scramble_',\n"," '_softmax',\n"," '_softmax_backward_data',\n"," '_sources',\n"," '_sparse_broadcast_to',\n"," '_sparse_broadcast_to_copy',\n"," '_sparse_bsc_tensor_unsafe',\n"," '_sparse_bsr_tensor_unsafe',\n"," '_sparse_compressed_tensor_unsafe',\n"," '_sparse_coo_tensor_unsafe',\n"," '_sparse_csc_tensor_unsafe',\n"," '_sparse_csr_prod',\n"," '_sparse_csr_sum',\n"," '_sparse_csr_tensor_unsafe',\n"," '_sparse_log_softmax_backward_data',\n"," '_sparse_mask_helper',\n"," '_sparse_softmax_backward_data',\n"," '_sparse_sparse_matmul',\n"," '_sparse_sum',\n"," '_stack',\n"," '_standard_gamma',\n"," '_standard_gamma_grad',\n"," '_storage_classes',\n"," '_string_classes',\n"," '_sync',\n"," '_tensor',\n"," '_tensor_classes',\n"," '_tensor_str',\n"," '_test_serialization_subcmul',\n"," '_to_cpu',\n"," '_to_functional_tensor',\n"," '_torch_cuda_cu_linker_symbol_op',\n"," '_transform_bias_rescale_qkv',\n"," '_transformer_encoder_layer_fwd',\n"," '_trilinear',\n"," '_unique',\n"," '_unique2',\n"," '_unpack_dual',\n"," '_use_cudnn_ctc_loss',\n"," '_use_cudnn_rnn_flatten_weight',\n"," '_utils',\n"," '_utils_internal',\n"," '_validate_sparse_bsc_tensor_args',\n"," '_validate_sparse_bsr_tensor_args',\n"," '_validate_sparse_compressed_tensor_args',\n"," '_validate_sparse_coo_tensor_args',\n"," '_validate_sparse_csc_tensor_args',\n"," '_validate_sparse_csr_tensor_args',\n"," '_values_copy',\n"," '_vmap_internals',\n"," '_weight_norm',\n"," '_weight_norm_interface',\n"," 'abs',\n"," 'abs_',\n"," 'absolute',\n"," 'acos',\n"," 'acos_',\n"," 'acosh',\n"," 'acosh_',\n"," 'adaptive_avg_pool1d',\n"," 'adaptive_max_pool1d',\n"," 'add',\n"," 'addbmm',\n"," 'addcdiv',\n"," 'addcmul',\n"," 'addmm',\n"," 'addmv',\n"," 'addmv_',\n"," 'addr',\n"," 'adjoint',\n"," 'affine_grid_generator',\n"," 'alias_copy',\n"," 'align_tensors',\n"," 'all',\n"," 'allclose',\n"," 'alpha_dropout',\n"," 'alpha_dropout_',\n"," 'amax',\n"," 'amin',\n"," 'aminmax',\n"," 'amp',\n"," 'angle',\n"," 'any',\n"," 'ao',\n"," 'arange',\n"," 'arccos',\n"," 'arccos_',\n"," 'arccosh',\n"," 'arccosh_',\n"," 'arcsin',\n"," 'arcsin_',\n"," 'arcsinh',\n"," 'arcsinh_',\n"," 'arctan',\n"," 'arctan2',\n"," 'arctan_',\n"," 'arctanh',\n"," 'arctanh_',\n"," 'are_deterministic_algorithms_enabled',\n"," 'argmax',\n"," 'argmin',\n"," 'argsort',\n"," 'argwhere',\n"," 'as_strided',\n"," 'as_strided_',\n"," 'as_strided_copy',\n"," 'as_tensor',\n"," 'asarray',\n"," 'asin',\n"," 'asin_',\n"," 'asinh',\n"," 'asinh_',\n"," 'atan',\n"," 'atan2',\n"," 'atan_',\n"," 'atanh',\n"," 'atanh_',\n"," 'atleast_1d',\n"," 'atleast_2d',\n"," 'atleast_3d',\n"," 'attr',\n"," 'autocast',\n"," 'autocast_decrement_nesting',\n"," 'autocast_increment_nesting',\n"," 'autograd',\n"," 'avg_pool1d',\n"," 'backends',\n"," 'baddbmm',\n"," 'bartlett_window',\n"," 'batch_norm',\n"," 'batch_norm_backward_elemt',\n"," 'batch_norm_backward_reduce',\n"," 'batch_norm_elemt',\n"," 'batch_norm_gather_stats',\n"," 'batch_norm_gather_stats_with_counts',\n"," 'batch_norm_stats',\n"," 'batch_norm_update_stats',\n"," 'bernoulli',\n"," 'bfloat16',\n"," 'bilinear',\n"," 'binary_cross_entropy_with_logits',\n"," 'bincount',\n"," 'binomial',\n"," 'bitwise_and',\n"," 'bitwise_left_shift',\n"," 'bitwise_not',\n"," 'bitwise_or',\n"," 'bitwise_right_shift',\n"," 'bitwise_xor',\n"," 'blackman_window',\n"," 'block_diag',\n"," 'bmm',\n"," 'bool',\n"," 'broadcast_shapes',\n"," 'broadcast_tensors',\n"," 'broadcast_to',\n"," 'bucketize',\n"," 'builtins',\n"," 'can_cast',\n"," 'candidate',\n"," 'cartesian_prod',\n"," 'cat',\n"," 'ccol_indices_copy',\n"," 'cdist',\n"," 'cdouble',\n"," 'ceil',\n"," 'ceil_',\n"," 'celu',\n"," 'celu_',\n"," 'cfloat',\n"," 'chain_matmul',\n"," 'chalf',\n"," 'channel_shuffle',\n"," 'channels_last',\n"," 'channels_last_3d',\n"," 'cholesky',\n"," 'cholesky_inverse',\n"," 'cholesky_solve',\n"," 'choose_qparams_optimized',\n"," 'chunk',\n"," 'clamp',\n"," 'clamp_',\n"," 'clamp_max',\n"," 'clamp_max_',\n"," 'clamp_min',\n"," 'clamp_min_',\n"," 'classes',\n"," 'classproperty',\n"," 'clear_autocast_cache',\n"," 'clip',\n"," 'clip_',\n"," 'clone',\n"," 'col_indices_copy',\n"," 'column_stack',\n"," 'combinations',\n"," 'compiled_with_cxx11_abi',\n"," 'complex',\n"," 'complex128',\n"," 'complex32',\n"," 'complex64',\n"," 'concat',\n"," 'conj',\n"," 'conj_physical',\n"," 'conj_physical_',\n"," 'constant_pad_nd',\n"," 'contiguous_format',\n"," 'conv1d',\n"," 'conv2d',\n"," 'conv3d',\n"," 'conv_tbc',\n"," 'conv_transpose1d',\n"," 'conv_transpose2d',\n"," 'conv_transpose3d',\n"," 'convolution',\n"," 'copysign',\n"," 'corrcoef',\n"," 'cos',\n"," 'cos_',\n"," 'cosh',\n"," 'cosh_',\n"," 'cosine_embedding_loss',\n"," 'cosine_similarity',\n"," 'count_nonzero',\n"," 'cov',\n"," 'cpp',\n"," 'cpu',\n"," 'cross',\n"," 'crow_indices_copy',\n"," 'ctc_loss',\n"," 'ctypes',\n"," 'cuda',\n"," 'cudnn_affine_grid_generator',\n"," 'cudnn_batch_norm',\n"," 'cudnn_convolution',\n"," 'cudnn_convolution_add_relu',\n"," 'cudnn_convolution_relu',\n"," 'cudnn_convolution_transpose',\n"," 'cudnn_grid_sampler',\n"," 'cudnn_is_acceptable',\n"," 'cummax',\n"," 'cummin',\n"," 'cumprod',\n"," 'cumsum',\n"," 'cumulative_trapezoid',\n"," 'default_generator',\n"," 'deg2rad',\n"," 'deg2rad_',\n"," 'dequantize',\n"," 'det',\n"," 'detach',\n"," 'detach_',\n"," 'detach_copy',\n"," 'device',\n"," 'diag',\n"," 'diag_embed',\n"," 'diagflat',\n"," 'diagonal',\n"," 'diagonal_copy',\n"," 'diagonal_scatter',\n"," 'diff',\n"," 'digamma',\n"," 'dist',\n"," 'distributed',\n"," 'distributions',\n"," 'div',\n"," 'divide',\n"," 'dot',\n"," 'double',\n"," 'dropout',\n"," 'dropout_',\n"," 'dsmm',\n"," 'dsplit',\n"," 'dstack',\n"," 'dtype',\n"," 'e',\n"," 'eig',\n"," 'einsum',\n"," 'embedding',\n"," 'embedding_bag',\n"," 'embedding_renorm_',\n"," 'empty',\n"," 'empty_like',\n"," 'empty_quantized',\n"," 'empty_strided',\n"," 'enable_grad',\n"," 'eq',\n"," 'equal',\n"," 'erf',\n"," 'erf_',\n"," 'erfc',\n"," 'erfc_',\n"," 'erfinv',\n"," 'exp',\n"," 'exp2',\n"," 'exp2_',\n"," 'exp_',\n"," 'expand_copy',\n"," 'expm1',\n"," 'expm1_',\n"," 'eye',\n"," 'fake_quantize_per_channel_affine',\n"," 'fake_quantize_per_tensor_affine',\n"," 'fbgemm_linear_fp16_weight',\n"," 'fbgemm_linear_fp16_weight_fp32_activation',\n"," 'fbgemm_linear_int8_weight',\n"," 'fbgemm_linear_int8_weight_fp32_activation',\n"," 'fbgemm_linear_quantize_weight',\n"," 'fbgemm_pack_gemm_matrix_fp16',\n"," 'fbgemm_pack_quantized_matrix',\n"," 'feature_alpha_dropout',\n"," 'feature_alpha_dropout_',\n"," 'feature_dropout',\n"," 'feature_dropout_',\n"," 'fft',\n"," 'fill',\n"," 'fill_',\n"," 'finfo',\n"," 'fix',\n"," 'fix_',\n"," 'flatten',\n"," 'flip',\n"," 'fliplr',\n"," 'flipud',\n"," 'float',\n"," 'float16',\n"," 'float32',\n"," 'float64',\n"," 'float_power',\n"," 'floor',\n"," 'floor_',\n"," 'floor_divide',\n"," 'fmax',\n"," 'fmin',\n"," 'fmod',\n"," 'fork',\n"," 'frac',\n"," 'frac_',\n"," 'frexp',\n"," 'frobenius_norm',\n"," 'from_dlpack',\n"," 'from_file',\n"," 'from_numpy',\n"," 'frombuffer',\n"," 'full',\n"," 'full_like',\n"," 'functional',\n"," 'fused_moving_avg_obs_fake_quant',\n"," 'futures',\n"," 'fx',\n"," 'gather',\n"," 'gcd',\n"," 'gcd_',\n"," 'ge',\n"," 'geqrf',\n"," 'ger',\n"," 'get_autocast_cpu_dtype',\n"," 'get_autocast_gpu_dtype',\n"," 'get_default_dtype',\n"," 'get_deterministic_debug_mode',\n"," 'get_device',\n"," 'get_file_path',\n"," 'get_float32_matmul_precision',\n"," 'get_num_interop_threads',\n"," 'get_num_threads',\n"," 'get_rng_state',\n"," 'gradient',\n"," 'greater',\n"," 'greater_equal',\n"," 'grid_sampler',\n"," 'grid_sampler_2d',\n"," 'grid_sampler_3d',\n"," 'group_norm',\n"," 'gru',\n"," 'gru_cell',\n"," 'gt',\n"," 'half',\n"," 'hamming_window',\n"," 'hann_window',\n"," 'hardshrink',\n"," 'has_cuda',\n"," 'has_cudnn',\n"," 'has_lapack',\n"," 'has_mkl',\n"," 'has_mkldnn',\n"," 'has_mps',\n"," 'has_openmp',\n"," 'has_spectral',\n"," 'heaviside',\n"," 'hinge_embedding_loss',\n"," 'histc',\n"," 'histogram',\n"," 'histogramdd',\n"," 'hsmm',\n"," 'hsplit',\n"," 'hspmm',\n"," 'hstack',\n"," 'hub',\n"," 'hypot',\n"," 'i0',\n"," 'i0_',\n"," 'igamma',\n"," 'igammac',\n"," 'iinfo',\n"," 'imag',\n"," 'import_ir_module',\n"," 'import_ir_module_from_buffer',\n"," 'index_add',\n"," 'index_copy',\n"," 'index_fill',\n"," 'index_put',\n"," 'index_put_',\n"," 'index_reduce',\n"," 'index_select',\n"," 'indices_copy',\n"," 'inf',\n"," 'inference_mode',\n"," 'init_num_threads',\n"," 'initial_seed',\n"," 'inner',\n"," 'inspect',\n"," 'instance_norm',\n"," 'int',\n"," 'int16',\n"," 'int32',\n"," 'int64',\n"," 'int8',\n"," 'int_repr',\n"," 'inverse',\n"," 'is_anomaly_enabled',\n"," 'is_autocast_cache_enabled',\n"," 'is_autocast_cpu_enabled',\n"," 'is_autocast_enabled',\n"," 'is_complex',\n"," 'is_conj',\n"," 'is_deterministic_algorithms_warn_only_enabled',\n"," 'is_distributed',\n"," 'is_floating_point',\n"," 'is_grad_enabled',\n"," 'is_inference',\n"," 'is_inference_mode_enabled',\n"," 'is_neg',\n"," 'is_nonzero',\n"," 'is_same_size',\n"," 'is_signed',\n"," 'is_storage',\n"," 'is_tensor',\n"," 'is_vulkan_available',\n"," 'is_warn_always_enabled',\n"," 'isclose',\n"," 'isfinite',\n"," 'isin',\n"," 'isinf',\n"," 'isnan',\n"," 'isneginf',\n"," 'isposinf',\n"," 'isreal',\n"," 'istft',\n"," 'jit',\n"," 'kaiser_window',\n"," 'kl_div',\n"," 'kron',\n"," 'kthvalue',\n"," 'layer_norm',\n"," 'layout',\n"," 'lcm',\n"," 'lcm_',\n"," 'ldexp',\n"," 'ldexp_',\n"," 'le',\n"," 'legacy_contiguous_format',\n"," 'lerp',\n"," 'less',\n"," 'less_equal',\n"," 'lgamma',\n"," 'library',\n"," 'linalg',\n"," 'linspace',\n"," 'load',\n"," 'lobpcg',\n"," 'log',\n"," 'log10',\n"," 'log10_',\n"," 'log1p',\n"," 'log1p_',\n"," 'log2',\n"," 'log2_',\n"," 'log_',\n"," 'log_softmax',\n"," 'logaddexp',\n"," 'logaddexp2',\n"," 'logcumsumexp',\n"," 'logdet',\n"," 'logical_and',\n"," 'logical_not',\n"," 'logical_or',\n"," 'logical_xor',\n"," 'logit',\n"," 'logit_',\n"," 'logspace',\n"," 'logsumexp',\n"," 'long',\n"," 'lstm',\n"," 'lstm_cell',\n"," 'lstsq',\n"," 'lt',\n"," 'lu',\n"," 'lu_solve',\n"," 'lu_unpack',\n"," 'manual_seed',\n"," 'margin_ranking_loss',\n"," 'masked_fill',\n"," 'masked_scatter',\n"," 'masked_select',\n"," 'matmul',\n"," 'matrix_exp',\n"," 'matrix_power',\n"," 'matrix_rank',\n"," 'max',\n"," 'max_pool1d',\n"," 'max_pool1d_with_indices',\n"," 'max_pool2d',\n"," 'max_pool3d',\n"," 'maximum',\n"," 'mean',\n"," 'median',\n"," 'memory_format',\n"," 'merge_type_from_type_comment',\n"," 'meshgrid',\n"," 'min',\n"," 'minimum',\n"," 'miopen_batch_norm',\n"," 'miopen_convolution',\n"," 'miopen_convolution_transpose',\n"," 'miopen_depthwise_convolution',\n"," 'miopen_rnn',\n"," 'mkldnn_adaptive_avg_pool2d',\n"," 'mkldnn_convolution',\n"," 'mkldnn_linear_backward_weights',\n"," 'mkldnn_max_pool2d',\n"," 'mkldnn_max_pool3d',\n"," 'mm',\n"," 'mode',\n"," 'moveaxis',\n"," 'movedim',\n"," 'msort',\n"," 'mul',\n"," 'multinomial',\n"," 'multiply',\n"," 'multiprocessing',\n"," 'mv',\n"," 'mvlgamma',\n"," 'name',\n"," 'nan',\n"," 'nan_to_num',\n"," 'nan_to_num_',\n"," 'nanmean',\n"," 'nanmedian',\n"," 'nanquantile',\n"," 'nansum',\n"," 'narrow',\n"," 'narrow_copy',\n"," 'native_batch_norm',\n"," 'native_channel_shuffle',\n"," 'native_dropout',\n"," 'native_group_norm',\n"," 'native_layer_norm',\n"," 'native_norm',\n"," 'ne',\n"," 'neg',\n"," 'neg_',\n"," 'negative',\n"," 'negative_',\n"," 'nested_tensor',\n"," 'nextafter',\n"," 'nn',\n"," 'no_grad',\n"," 'nonzero',\n"," 'norm',\n"," 'norm_except_dim',\n"," 'normal',\n"," 'not_equal',\n"," 'nuclear_norm',\n"," 'numel',\n"," 'obj',\n"," 'ones',\n"," 'ones_like',\n"," 'onnx',\n"," 'ops',\n"," 'optim',\n"," 'orgqr',\n"," 'ormqr',\n"," 'os',\n"," 'outer',\n"," 'overrides',\n"," 'package',\n"," 'pairwise_distance',\n"," 'parse_ir',\n"," 'parse_schema',\n"," 'parse_type_comment',\n"," 'pca_lowrank',\n"," 'pdist',\n"," 'per_channel_affine',\n"," 'per_channel_affine_float_qparams',\n"," 'per_channel_symmetric',\n"," 'per_tensor_affine',\n"," 'per_tensor_symmetric',\n"," 'permute',\n"," 'permute_copy',\n"," 'pi',\n"," 'pinverse',\n"," 'pixel_shuffle',\n"," 'pixel_unshuffle',\n"," 'platform',\n"," 'poisson',\n"," 'poisson_nll_loss',\n"," 'polar',\n"," 'polygamma',\n"," 'positive',\n"," 'pow',\n"," 'prelu',\n"," 'prepare_multiprocessing_environment',\n"," 'preserve_format',\n"," 'prod',\n"," 'profiler',\n"," 'promote_types',\n"," 'put',\n"," 'q_per_channel_axis',\n"," 'q_per_channel_scales',\n"," 'q_per_channel_zero_points',\n"," 'q_scale',\n"," 'q_zero_point',\n"," 'qint32',\n"," 'qint8',\n"," 'qr',\n"," 'qscheme',\n"," 'quantile',\n"," 'quantization',\n"," 'quantize_per_channel',\n"," 'quantize_per_tensor',\n"," 'quantize_per_tensor_dynamic',\n"," 'quantized_batch_norm',\n"," 'quantized_gru',\n"," 'quantized_gru_cell',\n"," 'quantized_lstm',\n"," ...]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","dir(torch.backends.mps.torch)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"4C4pZJtcQ_fm"},"outputs":[],"source":["import numpy as np\n","from math import floor\n","from torch.utils.data import Subset\n","\n","def downsample_cifar10_dataset(dataset, percentage, seed=42):\n","  \"\"\"\n","  Downsample the given dataset by only keeping a percentage of data.\n","\n","  Args:\n","    dataset: The dataset to downsample.\n","    percentage: The percentage of data to keep.\n","    seed: The seed to configure the random subset choice.\n","  \"\"\"\n","  rng = np.random.default_rng(seed)\n","\n","  index_to_keep = []\n","  for class_idx in range(len(dataset.class_to_idx)):\n","    class_mask = [target == class_idx for target in dataset.targets]\n","    indices_class = [i for i, x in enumerate(class_mask) if x]\n","    index_to_keep.extend(np.random.choice(indices_class, floor(percentage * len(indices_class)), replace=False))\n","  \n","  return Subset(dataset, index_to_keep)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"LNyDEvYeCzhR"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (2492916311.py, line 29)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn [2], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    transform =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["from torch.utils.data import DataLoader\n","from torchvision.datasets import CIFAR10\n","from torchvision.transforms import ToTensor, Compose, Normalize, Resize\n","\n","idx_to_class = {\n","    0: \"airplane\",\n","    1: \"automobile\",\n","    2: \"bird\",\n","    3: \"cat\",\n","    4: \"deer\",\n","    5: \"dog\",\n","    6: \"frog\",\n","    7: \"horse\",\n","    8: \"ship\",\n","    9: \"truck\"\n","}\n","\n","mean_cifar=[0.491, 0.482, 0.446]\n","std_cifar=[0.247, 0.243, 0.261]\n","\n","# --- START CODE HERE (02)\n","# Intantiate the transform and the datasets for training and testing\n","transform = Compose([\n","    Resize(32),\n","    ToTensor(),\n","    Normalize(mean_cifar, std_cifar)\n","])\n","dataset_train = CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n","dataset_test = CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n","# --- END CODE HERE\n","\n","dataset_train = downsample_cifar10_dataset(dataset_train, percentage=0.2)\n","dataset_test = downsample_cifar10_dataset(dataset_test, percentage=0.2)\n","\n","# --- START CODE HERE (03)\n","# Intantiate the dataloaders for training and testing\n","train_dataloader = DataLoader(dataset_train, batch_size=128, shuffle=True, )\n","test_dataloader = \n","# --- END CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"0JHMFOkZcy-Q"},"source":["### Visualize the data\n","\n","Below you can visualize various images from the CIFAR10 dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WZo1tbqHsSM"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision.transforms.functional as F\n","from torchvision.utils import make_grid\n","\n","def show(imgs):\n","  \"\"\"\n","  Show input images.\n","\n","  Args:\n","    imgs: the images to show\n","  \"\"\"\n","\n","  if not isinstance(imgs, list):\n","    imgs = [imgs]\n","  fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(15, 15))\n","  for i, img in enumerate(imgs):\n","    img = img.detach()\n","    img = F.to_pil_image(img)\n","    axs[0, i].imshow(np.asarray(img))\n","    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","def denormalize_tensor(tensor, mean, std):\n","  \"\"\"\n","  Denormalize a tensor.\n","\n","  Args:\n","    tensor: the tensor to denormalize.\n","    mean: the mean that normalized.\n","    std: the std that normalized.\n","  \"\"\"\n","  \n","  mean = torch.tensor(mean).view((1, 3, 1, 1))\n","  std = torch.tensor(std).view((1, 3, 1 , 1))\n","  return tensor * std + mean\n","\n","random_train_samples = [dataset_train[idx] for idx in np.random.choice(len(dataset_train), 16)]\n","random_train_X = torch.stack([sample[0] for sample in random_train_samples])\n","random_train_y = [idx_to_class[sample[1]] for sample in random_train_samples]\n","train_grid = make_grid(denormalize_tensor(random_train_X, mean_cifar, std_cifar))\n","\n","print(f\"Shape of train images: {random_train_X[0].shape}.\")\n","print(f\"Labels:\\n{random_train_y}.\\n\")\n","print(f\"Grid of train images:\")\n","show(train_grid)"]},{"cell_type":"markdown","metadata":{"id":"hAwxSzh7gbqs"},"source":["### Implement custom model"]},{"cell_type":"markdown","metadata":{"id":"CKIGqeN56gN-"},"source":["Here, we will implement the custom model following the [Sequential model](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) from Pytorch.\n","\n","You will need to feed to the Sequential model the layers that define your model. These layers must follow the architecture detailed above. The various layers required are available in Pytorch and can be found in the Neural Network, or [nn](https://pytorch.org/docs/stable/nn.html), package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4znmTlDFNqJ"},"outputs":[],"source":["# --- START CODE HERE (04)\n","# Import the layers to implement the model\n","\n","# --- END CODE HERE\n","\n","def make_custom_model(): # Function in case you need to reinitialize model.\n","  \"\"\"\n","  Make the custom model.\n","  \"\"\"\n","\n","  layers = [\n","    # --- START CODE HERE (05)\n","    # Instantiate the various layers composing our model.\n"," \n","    # --- END CODE HERE\n"," ]\n","\n","  # --- START CODE HERE (06)\n","  # Feed the layers to the Sequential class.\n","\n","  # --- END CODE HERE\n","  return model\n","\n","model = make_custom_model()\n","model"]},{"cell_type":"markdown","metadata":{"id":"Gt4BV39BgeyE"},"source":["### Train custom model"]},{"cell_type":"markdown","metadata":{"id":"uzrZDe-v734u"},"source":["#### Helper functions\n","\n","We define several helper functions to compute the accuracy, and display the loss and accuracy to visualize our trainings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ku2gCLjjN_jr"},"outputs":[],"source":["def display_losses_and_accuracies(train_loss, test_loss, train_accuracy, test_accuracy):\n","  \"\"\"\n","  Display the training and testing loss and accuracies.\n","\n","  Args:\n","    train_loss: the training loss list.\n","    test_loss: the testing loss list.\n","    train_accuracy: the training accuracy list.\n","    test_accuracy: the testing accuracy list.\n","  \"\"\"\n","\n","  plt.subplot(1,2,1)\n","  plt.plot(train_loss, 'r')\n","  plt.plot(test_loss, 'g--')\n","  plt.xlabel('# epoch')\n","  plt.ylabel('loss')\n","  plt.grid(True)\n","\n","  plt.subplot(1,2,2)\n","  plt.plot(train_accuracy, 'r')\n","  plt.plot(test_accuracy, 'g--')\n","  plt.xlabel('# epoch')\n","  plt.ylabel('accuracy')\n","  plt.grid(True)\n","  plt.show()\n","\n","\n","def F_computeAccuracy(preds, y):\n","    \"\"\"Compute the accuracy\n","    \n","    Parameters\n","    ----------\n","    preds: (m, 1)\n","        predicted value by the MLP\n","    y: (m, 1)\n","        ground-truth class to predict\n","    \"\"\"\n","    \n","    m = y.shape[0]\n","    if preds.shape[1] == 1:\n","      hard_preds = (preds > 0.5).to(torch.int64)\n","    else:\n","      hard_preds = (torch.argmax(preds, 1)).to(torch.int64)\n","    return torch.sum(hard_preds==y) / m"]},{"cell_type":"markdown","metadata":{"id":"6FE9aidg8Dq4"},"source":["#### Training loop\n","\n","To reuse our training loop, we will define a function that takes as an argument:\n","- the model\n","- the loss function\n","- the optimizer\n","- the training and testing dataloaders\n","- the number of epochs to perform.\n","\n","In this loop, you will take care of \n","- making the iterator over the dataloaders\n","- [putting on the cuda device](https://pytorch.org/docs/stable/notes/cuda.html) the batch of data for training and testing\n","\n","The rest of the loop is the same as for the first TP:\n","- computation of the predictions by passing through the models\n","- loss computation based on predictions and targets\n","- accuracy computation and metrics storage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeEMuMHn8fV_"},"outputs":[],"source":["def training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, num_epochs):\n","  train_loss_epoch, train_accuracy_epoch = [], []\n","  test_loss_epoch, test_accuracy_epoch = [], []\n","\n","  for num_epoch in range(1, nb_epoch + 1):\n","    model.train()\n","    train_loss_iter, train_accuracy_iter = [], []\n","    test_loss_iter, test_accuracy_iter = [], []\n","\n","    # --- START CODE HERE (07)\n","    # Iterate over the train dataloader and put the batch data on cuda device.\n","    for _ in _:\n","      X, y = \n","    # --- END CODE HERE\n","\n","      # --- Forward\n","      train_preds = model(X)\n","      loss = loss_fn(train_preds, y)\n","\n","      accuracy = F_computeAccuracy(train_preds, y)\n","      \n","      # --- Store results on train\n","      train_loss_iter.append(loss.item())\n","      train_accuracy_iter.append(accuracy)\n","      \n","      # --- Backward\n","      optimizer.zero_grad()\n","      loss.backward()\n","      \n","      # --- Update parameters\n","      optimizer.step()\n","\n","    with torch.no_grad():\n","      model.eval()\n","      # --- START CODE HERE (08)\n","      # Iterate over the test dataloader and put the batch data on cuda device.\n","      for _ in _:\n","        X, y = \n","        \n","      # --- END CODE HERE\n","\n","        # --- Store results on test\n","        test_preds = model(X)\n","        loss = loss_fn(test_preds, y)\n","        accuracy = F_computeAccuracy(test_preds, y)\n","        test_loss_iter.append(loss.item())    \n","        test_accuracy_iter.append(accuracy)\n","\n","    train_loss_epoch.append(torch.mean(torch.tensor(train_loss_iter)))  \n","    train_accuracy_epoch.append(torch.mean(torch.tensor(train_accuracy_iter)))  \n","    test_loss_epoch.append(torch.mean(torch.tensor(test_loss_iter)))  \n","    test_accuracy_epoch.append(torch.mean(torch.tensor(test_accuracy_iter)))  \n","    \n","    print(\"epoch: {0:d} (loss: train {1:.2f} test {2:.2f}) (accuracy: train {3:.2f} test {4:.2f})\".format(num_epoch, train_loss_epoch[-1], test_loss_epoch[-1], train_accuracy_epoch[-1], test_accuracy_epoch[-1]))\n","\n","  return train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch"]},{"cell_type":"markdown","metadata":{"id":"j-HkMh18-CsX"},"source":["Now that the dataloading, the model definition, and the training loop are done we can train our model.\n","\n","Before that, we need to instantiate our optimizer and our loss function.\n","Also to speed-up training, as explained before, we will use the [cuda device](https://pytorch.org/docs/stable/notes/cuda.html), which is the GPU.\n","\n","All modules should be put on the cuda device, so in our case:\n","- the model\n","- the loss function\n","\n","In the training loop, we already took care of having the data on the cuda device so we will not encounter an error.\n","\n","You should observe overfitting during training (low training loss and increasing testing loss). Later in the TP we will add data augmentations to reduce this issue."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TMQHGgCHK2z"},"outputs":[],"source":["# --- START CODE HERE (09)\n","# Import the loss and optimizer classes.\n","\n","# --- END CODE HERE\n","\n","# Run over epochs\n","nb_epoch = 20\n","\n","model = make_custom_model()\n","\n","# --- START CODE HERE (10)\n","# Put the model on cuda device and initialize the optimizer and the loss function to put on cuda aswell\n","optimizer = \n","loss_fn = \n","# --- END CODE HERE\n","\n","train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n","  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch)\n","\n","display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"]},{"cell_type":"markdown","metadata":{"id":"LiwHJT6FRQ9M"},"source":["## Train the CNN backbone ResNet-18\n","\n","\n","In this section, we will train a [ResNet-18](https://arxiv.org/pdf/1512.03385.pdf) model that has proven effective to classification tasks.\n","\n","We will train in two different ways our ResNet-18:\n"," - we will simply take a ResNet-18 randomly initialized backbone and train it for a few epochs on CIFAR10.\n","\n"," - we will instantiate a ResNet-18 that has first been pretrained on ImageNet and finetune it on CIFAR10 for a few epochs.\n"]},{"cell_type":"markdown","metadata":{"id":"nSMwABTYVi39"},"source":["### Load data\n","\n","The ResNet-18 architecture has been proposed to solve the ImageNet classification problem which has a large resolution in comparison of CIFAR10 (around 256x256 vs 32x32). \n","\n","The first part of ResNet is called stem and aims to reduce the dimension of the input. To properly make use of this backbone, we can either change the stem, or resize our input to match Imagenet. You can find the architecture implementation [here](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py).\n","\n","We will go for the latter as we will compare our randomly initialized backbone to the finetuned one that has to keep the same stem to properly initialized its weights.\n","\n","Based on this requirement, we will define a new training and testing dataset transform that will be composed of:\n","- Resize the image to 224x224\n","- Transform the image to Tensor\n","- Normalize the data according to CIFAR10 statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owgMdv8pWXP5"},"outputs":[],"source":["# --- START CODE HERE (11)\n","# Import the transform and instantiate the right transform.\n","\n","# --- END CODE HERE\n","\n","dataset_train = CIFAR10('.', train=True, transform=transform, download=True)\n","dataset_test = CIFAR10('.', train=False, transform=transform, download=True)\n","\n","dataset_train = downsample_cifar10_dataset(dataset_train, percentage=0.2)\n","dataset_test = downsample_cifar10_dataset(dataset_test, percentage=0.2)\n","\n","train_dataloader = DataLoader(dataset_train, batch_size=128, shuffle=True, drop_last=True, num_workers=2)\n","test_dataloader = DataLoader(dataset_test, batch_size=128, shuffle=False, drop_last=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"lXeYrn2hXpcR"},"source":["### Instantiate the model\n","\n","The ResNet model has been proposed to perform on the ImageNet dataset, therefore the last fully connected layer to make class prediction has an output dimension of 1000, which is the number of classes of ImageNet.\n","\n","To properly use ResNet on CIFAR10, we must change this last fully connected layer to have an output dimension of 10. We will use the Torchvision [Resnet18 implementation](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html).\n","\n","We also want to be able to instantiate a model that is initialized in two ways:\n","- Random: to perform training from scratch\n","- Pretrained: transfer learning from the ImageNet pretrained weights.\n","\n","For transfer learning, we will have the choice to either finetune the whole backbone or only the last fully connected layer. The former generally provides better result as the whole backbone gets to specialize to the task but the latter has the advantage to require fewer computational resources, and is better for very small datasets.\n","\n","For only training the last layer, we will have to inform Pytorch to not optimize other parameters. This can be done by iterating over the [parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters) and changing their [requires_grad](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html) value.\n","\n","All these requirements need to be implemented in the function below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzqF-BWFXNcp"},"outputs":[],"source":["# --- START CODE HERE (12)\n","# Import the resnet model\n","\n","# --- END CODE HERE\n","\n","def get_resnet18(pretrained: bool = False, output_dim: int = 1000, only_train_fc: bool = False):\n","  if pretrained:\n","    weights = ResNet18_Weights.DEFAULT\n","  else:\n","    weights = None\n","\n","  # --- START CODE HERE (13)\n","  # Configure the resnet model and update its fully connected layer\n","  \n","  # --- END CODE HERE\n","\n","  if only_train_fc:\n","    # --- START CODE HERE (14)\n","    # Only keep the fully connected layer parameters trainable.\n","    \n","    pass\n","    # --- END CODE HERE\n","\n","  return model\n","\n","get_resnet18(False, 10)"]},{"cell_type":"markdown","metadata":{"id":"4hNMYvuvgkJ8"},"source":["### Train the randomly initialized ResNet-18 backbone\n","\n","Use the previous function to instantiate the randomly initialized ResNet-18.\n","\n","You should observe that the training loss keeps decreasing but the test performance seeems to reach some kind of plateau or decrease. This is an exemple of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Rjj8YpbY6EB"},"outputs":[],"source":["# Run over epochs\n","nb_epoch = 10\n","\n","# --- START CODE HERE (15)\n","# Instantiate the randomly initialised resnet18.\n","model = \n","# --- END CODE HERE\n","\n","# Initialize the optimizer and the loss function\n","optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n","loss_fn = CrossEntropyLoss().cuda()\n","\n","\n","train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n","  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch)\n","\n","display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"]},{"cell_type":"markdown","metadata":{"id":"3LKbcBingov2"},"source":["### Transfer learning of a pretrained ResNet-18 backbone"]},{"cell_type":"markdown","metadata":{"id":"9ktnE33phhQD"},"source":["In this section we will use the pretained ResNet-18 on Imagenet to perform either linear classifier training or finetuning of the whole backbone.\n","\n","#### Linear classifier training\n","\n","Instantiate the ResNet-18 with the ImageNet pretrained weights using the function we implemented but for training only the linear classifier.\n","\n","You should observe a large improvement over random initialization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a2KNvK6ZCje"},"outputs":[],"source":["# Run over epochs\n","nb_epoch = 10\n","\n","# --- START CODE HERE (16)\n","# Instantiate the pretrained resnet18 with only the fully connected layer trained.\n","model = \n","# --- END CODE HERE\n","\n","# Initialize the optimizer and the loss function\n","optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n","loss_fn = CrossEntropyLoss().cuda()\n","\n","train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n","  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch)\n","\n","display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"]},{"cell_type":"markdown","metadata":{"id":"4-vKGHcl3kYc"},"source":["#### Complete finutining\n","\n","Instantiate the ResNet-18 with ImageNet pretrained weights using the function we implemented for training the whole backbone.\n","\n","You should observe an improvement over linear classifier training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxOReyY0j33I"},"outputs":[],"source":["# Run over epochs\n","nb_epoch = 10\n","\n","# --- START CODE HERE (17)\n","# Instantiate the pretrained resnet18 with the whole backbone trained.\n","model = \n","# --- END CODE HERE\n","\n","optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n","loss_fn = CrossEntropyLoss().cuda()\n","\n","train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n","  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch)\n","\n","display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"]},{"cell_type":"markdown","metadata":{"id":"XI69Ld1aiL3Z"},"source":["## Data Augmentation\n","\n","To improve the generalization of our model and reduce risk of overfitting on the data, we can make use of Data Augmentation.\n","\n","The idea of Data Augmentation is to perform various transformations of our training data that enhance in some way the number of training data but also can make our representations invariant to various changes.\n","\n","The most common data augmentations used for classification are:\n","- random resized crop: that randomly take a subpart of the image and resize the image to the input size\n","- random horizontal flip: that randomly flip the image horizontally\n","\n","Some stronger augmentations have been used for either classification or self-supervised learning. Among them we can cite:\n","- color jittering: change the brightness, contrast, saturation and hue of an image\n","- gaussian blur: apply a gaussian kernel to the image\n","- grayscale or color dropping: lose color information of the image\n","\n","All these augmentations are available in [Torchvison](https://pytorch.org/vision/stable/transforms.html)."]},{"cell_type":"markdown","metadata":{"id":"E7JPqc-hfOIB"},"source":["### Data Augmentation visualization\n","\n","We provide you the code to be able to visualize augmentations along with the raw images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_KmTKPIgWOn"},"outputs":[],"source":["def get_cifar_train_and_test_datasets(train_transform=None, test_transform=None, percentage=0.2):\n","  \"\"\"\n","  Construct CIFAR train and test datasets.\n","\n","  Args:\n","    train_transform: transform for training dataset\n","    test_transform: transform for testing dataset\n","    percentage: percentage of data to keep \n","  \"\"\"\n","\n","  dataset_train = CIFAR10('.', train=True, transform=train_transform, download=True)\n","  dataset_test = CIFAR10('.', train=False, transform=test_transform, download=True)\n","\n","  dataset_train = downsample_cifar10_dataset(dataset_train, percentage=percentage)\n","  dataset_test = downsample_cifar10_dataset(dataset_test, percentage=percentage)\n","\n","  return dataset_train, dataset_test\n","\n","\n","def get_cifar_train_and_test_dataloaders(dataset_train, dataset_test, batch_size=128):\n","  \"\"\"\n","  Construct CIFAR train and test dataloaders.\n","\n","  Args:\n","    dataset_train: training dataset\n","    dataset_test: testing dataset\n","    batch_size: batch size for both dataloaders\n","  \"\"\"\n","\n","  train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n","  test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)\n","\n","  return train_dataloader, test_dataloader\n","\n","\n","def visualize_transform(transform = ToTensor(), mandatory_transform = ToTensor(), nb_images_to_visualize = 10):\n","  \"\"\"\n","  Visualize data augmentations and image before the data augmentation.\n","  \n","  Args:\n","    transform: transform to visualize (Optionaly a resize + other transforms)\n","    mandatory_transform: If transform contains resize, also give resize transform to correctly visualize the images.\n","    nb_images_to_visualize: The number of images to visualize.\n","  \"\"\"\n","\n","  dataset_train, _ = get_cifar_train_and_test_datasets(train_transform=None, test_transform=None)\n","\n","  raw_images = [dataset_train[i][0] for i in range(nb_images_to_visualize)]\n","  transformed_images = [transform(image) for image in raw_images]\n","  transformed_images_mandatory = [mandatory_transform(image) for image in raw_images] if mandatory_transform is not None else raw_images\n","\n","  transformed_images = torch.stack(transformed_images)\n","  transformed_images_mandatory = torch.stack(transformed_images_mandatory)\n","\n","  grid = make_grid(torch.cat((transformed_images_mandatory, transformed_images)), nrow=nb_images_to_visualize)\n","\n","  show(grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JfZT7sXYk_bD"},"outputs":[],"source":["from torchvision.transforms import ColorJitter \n","\n","transform = Compose([\n","    Resize(224),\n","    ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n","    ToTensor()\n","])\n","\n","resize_transform = Compose([\n","    Resize(224),\n","    ToTensor()\n","])\n","\n","visualize_transform(transform, resize_transform)"]},{"cell_type":"markdown","metadata":{"id":"5qy871Ac0Lod"},"source":["### Train our custom CNN with Data Augmentations\n","\n","Now, we will train our randomly initialized custom model with some new transforms.\n","\n","The first set of data augmentation we will try is the common for classification:\n","- random horizontal flip: with a probability of 0.5\n","\n","Then we will add [randomly](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomApply.html) color jittering and grayscale with the parameters:\n","- 0.8 probability to add color jitter\n","- 0.4 for brightness, contrast, saturation\n","- 0.1 for hue\n","- 0.2 probability to add grayscale\n","\n","On top of these data augmentations, do not forget to transform the image to tensor and normalize the data according to CIFAR statistics.\n","\n","You should observe that the data augmentations improve performance and reduce overfitting. However, to better see this, you might want to increase the number of epochs to train. Indeed, it is harder for the model to train with the data augmentations and the improvement is generally seen for longer training. Also the random initialization can be better for some configurations and the effect of better initialization is reduced for longer training."]},{"cell_type":"markdown","metadata":{"id":"_aTXK0_7-vP5"},"source":["#### Classical Data Augmentation\n","\n","Train the custom CNN with the classical data augmentation, you should observe less overfitting than before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQTCIkovMGRX"},"outputs":[],"source":["# --- START CODE HERE (18)\n","# Import the transforms and instantiate the classical set of data augmentation for classification.\n","\n","train_transform = \n","# --- END CODE HERE\n","\n","test_transform = Compose([\n","    ToTensor(), \n","    Normalize([0.491, 0.482, 0.446], [0.247, 0.243, 0.261])\n","])\n","\n","dataset_train, dataset_test = get_cifar_train_and_test_datasets(train_transform=train_transform, test_transform=test_transform, percentage=0.2)\n","train_dataloader, test_dataloader = get_cifar_train_and_test_dataloaders(dataset_train, dataset_test, 128)\n","\n","# Run over epochs\n","nb_epoch = 20\n","\n","model = make_custom_model()\n","model.cuda()\n","\n","optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n","loss_fn = CrossEntropyLoss().cuda()\n","\n","train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n","  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch)\n","\n","display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"]},{"cell_type":"markdown","metadata":{"id":"J9efDcMm-zUQ"},"source":["#### Stronger Data Augmentation\n","\n","Train the custom CNN with the stronger data augmentation, you should observe less overfitting than the classical data augmentation and hopefully slightly better results. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2qs7o0dMeek"},"outputs":[],"source":["# --- START CODE HERE (19)\n","# Import the transforms and instantiate the stronger set of data augmentation.\n","\n","train_transform = \n","# --- END CODE HERE\n","\n","test_transform = Compose([\n","    ToTensor(), \n","    Normalize([0.491, 0.482, 0.446], [0.247, 0.243, 0.261])\n","])\n","\n","dataset_train, dataset_test = get_cifar_train_and_test_datasets(train_transform=train_transform, test_transform=test_transform, percentage=0.2)\n","train_dataloader, test_dataloader = get_cifar_train_and_test_dataloaders(dataset_train, dataset_test, 128)\n","\n","# Run over epochs\n","nb_epoch = 20\n","\n","model = make_custom_model()\n","model.cuda()\n","\n","optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n","loss_fn = CrossEntropyLoss().cuda()\n","\n","train_loss_epoch, train_accuracy_epoch, test_loss_epoch, test_accuracy_epoch = \\\n","  training_loop(model, loss_fn, optimizer, train_dataloader, test_dataloader, nb_epoch)\n","\n","display_losses_and_accuracies(train_loss_epoch, test_loss_epoch, train_accuracy_epoch, test_accuracy_epoch)"]},{"cell_type":"markdown","metadata":{"id":"lXgre-nGB8OM"},"source":["## BONUS\n","\n","We tested various things using the Resnet backbone:\n","- training from random initialization\n","- finetuning the whole backbone or training the last layer\n","\n","If you want to perform more tests, you can try to use differents backbones such as VGG, or look at data augmentations.\n","\n","There are few parameters that you can also try to tune to improve performance such as:\n","- the batch size\n","- the optimizer\n","- the learning rate\n","- longer training\n","\n","You can try to [visualize your learned kernel weights](https://github.com/utkuozbulak/pytorch-cnn-visualizations#convolutional-neural-network-filter-visualization).\n","\n","Also you can have a look to various Kaggle competitions on Computer Vision !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3dlYGR0K0dy"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOKtTD+yCQJgBAdvATQQIhN","collapsed_sections":["lXeYrn2hXpcR"],"provenance":[{"file_id":"1JwBCclnXa7CC2RkZX4lNPgHnU7u0c3pw","timestamp":1664103624414}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 ('machine_learning')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"d38c4cc43721ac0d114dc8b1e646949e6883c7f0d964ef46ebccf5a0ec111b05"}}},"nbformat":4,"nbformat_minor":0}
